{"docstore/data": {"929d3638-607b-4906-92ed-38af34a5c687": {"__data__": {"id_": "929d3638-607b-4906-92ed-38af34a5c687", "embedding": null, "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/", "category": "history"}, "excluded_embed_metadata_keys": ["url"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "903007c0-81dc-4286-bafe-e76a64777a77", "node_type": "4", "metadata": {"url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/", "category": "history"}, "hash": "6986d634b1a1d4d315e925b85b2c7149e1d9d2f42d1a624a631483198c4cc3ab", "class_name": "RelatedNodeInfo"}}, "text": "Alongside the rise of Large Language Models (LLM), has risen a swath of programming frameworks that aim to make it easier to build applications on top of them, such as LangChain and LlamaIndex . LlamaIndex is a programming framework that aims to simplify the development of LLM-enabled applications that leverage Retrieval Augmented Generation (RAG). A core conceptual component to understanding the mechanisms behind LlamaIndex is that of query Response Modes . LlamaIndex has 5 built-in Response Modes: compact, refine, tree_summarize, accumulation, and simple_summarize. In this article, I will: To evaluate each mode, I put them to the test in helping answer a question that has plagued the world since that fateful day in November 1963: Who shot President John F. Kennedy? I will demonstrate how each Response Mode works by asking LlamaIndex to summarize the conclusions of the \u2018 Warren Commission Report \u2019, the 250,000-word Congressional report that investigated the assassination of President Kennedy. This document far exceeds the token limit for GPT-4 and most other mainstream LLMs and requires the use of RAG techniques for an LLM to answer questions on it using only passed in contextual data (not training data). The concept of Retrieval Augmented Generation (RAG) describes an approach to allow an LLM to answer questions based on data that it wasn\u2019t originally trained on. In order to do this, an LLM must be fed this data as part of the prompt, which is generally referred to as \u2018context\u2019. However, LLMs have limited input size restrictions (also called token input size), which make it impossible and impractical to pass large datasets, such as the Warren Report, in their entirety via the prompt. Instead, with the RAG-approach, a query to an LLM is broken into two parts: a \u2018retrieval step\u2019 and then a \u2018generation step\u2019. The \u2018retrieval step\u2019 attempts to identify the portions of the original dataset that are most relevant to a user supplied query and to only pass this subset of data to an LLM, alongside the original query, as part of the \u2018generative step\u2019. Essentially, RAG is a mechanism to work within the input size restrictions of LLMs by only including in the prompt the most relevant parts of the dataset needed to answer a query. Usually, the \u2018retrieval\u2019 portion of RAG utilizes tried-and-true semantic search algorithms such as Cosine Similarity , alongside a Vector Databases to perform this step. For the purposes of this article and the testing of the Response Modes, the question I am seeking to get an answer to is: \u201cWhat are the conclusions of the Warren Report?\u201d For the purposes of this article, in the retrieval step I set my code to return the top 5 most relevant \u2018chunks\u2019 of data that relate to my original query from the Warren Report as part of the Cosine similarity algorithm. These 5 chunks of data are then passed forward to the LLM, which is where LlamaIndex Response Modes come into play. When building an LLM enabled application that utilizes RAG techniques, it is still likely that the subset of data returned as part of the retrieval step, which are normally referred to as \u2018chunks\u2019, will still be too large to fit within the input token limit for a single LLM call. Most likely, multiple calls will need to be made to the LLM in order to derive a single answer that utilizes all of the retrieved chunks. LlamaIndex Response Modes govern the various approaches that can be used to break down, sequence and combine the results of multiple LLM calls with these chunks to return a single answer to the original query. As of writing, there are 5 basic Response Modes that can be used when querying with LlamaIndex. In evaluating the 5 different Response Modes, I utilize the following frameworks and tools: For each test, I pose the same question: \u201cWhat are the main conclusions of the Warren Report regarding the assassination of President Kennedy?\u201d The following is the Python code I used to evaluate each Response Mode, this one configured to use the compact Response Mode: Note also that in my call to initialize the query_engine in LlamaIndex, I set the similarity_top_k parameter to 5, which tells LlamaIndex to return the top 5 chunks of data that are semantically similar to the query as part of the retrieval step. For all Response Modes, the same 5 chunks of text are returned from the Warren Commission Report, which are available for you to view in the table below: The Response Mode called compact is the default mode used by LlamaIndex if none is specified. The way the compact mode works is that for each chunk that is returned from the retrieval step, LlamaIndex concatenates as many of those chunks together into the largest possible string that fits into a single prompt to the GPT-4 LLM. In our example, the first 4 chunks of matched text fit into the context window for a single GPT-4 call, which means that it requires 2 LLM calls to answer our query on the conclusion of the Warren Report. The 1st call made using the compact Response Mode always uses the text_qa_template prompt: LlamaIndex then takes the answer returned to this prompt, the next concatenated set of chunks (in our case simply Chunk 5) and passes it to the LLM along with the last few sentences of Chunk 4 using the refine_template prompt: As you can see, the compact Response Mode doesn\u2019t answer the question at anything resembling a coherent answer. In fact, the LLM ends up throwing up its hands and doing a spot on rendition of a high school student fumbling their way trying to answer a question they simply have no clue about. The likely reason why the compact Response Mode was unable to answer the question is that the first 4 chunks of text from the Warren Report don\u2019t actually contain the conclusions of the report, which only appears in Chunk 5 (which oddly has a lower Cosine similarity score than the proceeding chunks). Thus, the structure of the refine_template is such that if the first set of calls has gone down the wrong path, it\u2019s difficult for the final prompt to steer the LLM back onto the right track. You can see the full-text log for both of the LLM calls made with the compact Response Mode below: The second Response Mode that LlamaIndex provides is refine . The refine mode is very similar to the compact Response Mode, except that instead of attempting to concatenate as many chunks as it can to maximize the use of the LLM token limit, LlamaIndex only include 1 chunk of retrieved data for each LLM call. Starting with the text_qa_template , LlamaIndex passes in Chunk 1 to the LLM. After that, LlamaIndex then progresses sequentially through each of the remaining chunks one at a time; with each subsequent LLM call using the refine_template to build upon the answer returned from the answer returned from the previous chunk. While the compact Response Mode tried to obfuscate it\u2019s inability to answer the question behind a wall of hand-waving text, the refine Response Mode yields a much more succinct, yet equally useless answer. Much like the compact mode, the refine is much more likely to not be able to properly answer the question when the initial chunks passed to it are less relevant. In our case, the first chunk of data from the Warren Report is the introduction and forward parts of the report which do not contain any conclusions whatsoever, which is likely why it was never able to return a proper answer to our query. You can see the full-text log for all of the LLM calls made with the refine Response Mode below: The third, and by far most effective, Response Mode is tree_summarize. The astute reader might surmise from the use of the word \u2018tree\u2019 that there is a recursive property to this response mode, and they would be correct. The tree_summarize mode in its base case makes a series of LLM calls that concatenate chunks of retrieved data so that it maximizes the input token limit for the LLM. It then takes the outputs of each of these base case responses, and then passes them together to the LLM and instructs it to derive an answer using those initial answers as context. For anyone familiar working with LangChain, the tree_summarize Response Mode is essentially the same as LangChain\u2019s MapReduceDocumentChain . In our analysis of the Warren Report, the tree_summarize mode required 3 calls to the LLM, 2 for processing the 5 chunks of data, and then 1 for combining the answers from the first 2. The prompt template used by the tree_summarize Response Mode is quite simple and the same each time, regardless if we are working at the \u2018leaf\u2019 level or higher up in the processing tree: After making a similar call that includes only Chunk 5, tree_summarize then uses the following prompt template to combine the answers: The tree_summarize response mode nails the answer and delivers a thoughtful and complete summary of the findings of the Warren Report that correctly identify Lee Harvey Oswald as the assassin, while also in the same breath, disabusing any notion of a conspiracy or second shooter. Whereas the refine and compact modes were led astray by the irrelevance of the first set of chunks they analyzed, tree_summarize mode overcomes this as it is uses a map-reduce pattern to have the LLM independently analyze each concatenated chunk of data and then in a separate prompt combine the outputs of those into a single answer. You can see the full-text log for all of the LLM calls made using the tree_summarize Response Mode below: The accumulate Response Mode is quite simple, LlamaIndex makes 1 call per retrieved chunk and then returns every \u2018non-null\u2019 answer together as an array of answers. For each of the calls that LlamaIndex makes in accumulate mode, it uses a similar template to tree_summarize : The returned result from LlamaIndex is an array of strings of length 2, containing the responses returned from the LLM for chunks 4 and 5. The answers for chunks 1,2,3 are not included in this result, because for each of those calls, the LLM returned the logical equivalent of a \u2018null\u2019 response in the form of 'The context does not provide information on the main conclusions of the Warren Report regarding the assassination of President Kenne dy.\u2019 The accumulate mode doesn\u2019t so much answer the question, but instead returns n answers to the question with each of the answers being scoped simply to the context chunk passed to the LLM in that call. It is the responsibility then of the calling application to take the list of answers to produce an actual final answer to the query. \u2018Accumulate\u2019 doesn\u2019t work well for something like the Warren Report where each chunk doesn\u2019t necessarily contain the complete information to answer the question. You can see the full-text log for all of the LLM calls made using the accumulate Response Mode below: The final LlamaIndex Response Mode is simple_summarize . This perhaps the most basic and straightforward of the response modes. In this mode, LlamaIndex truncates all text chunks so that all chunks can be concatenated and passed into the LLM in a single call. No matter how many chunks are retrieved, there will only ever be a single call made to the LLM. In our example, the prompt template that is used in simple_summarize mode looks like: Surprisingly, the answer provided by simple_summarize is not bad and almost as complete as the one provided by tree_summarize However, I would attribute this more to luck then a structural advantage of the mechanism. With the simple_summarize mode, as the number of chunks returned from the retrieval step goes up, its likely that the ultimate answer returned will decrease in quality due to the knowledge being lost in the truncated segments of each chunk. You can view the full-text log of the prompt made in the simple_summarize test here . Looking at the results of my tests, the tree_summarize Response Mode returned the most comprehensive and complete answer to the question \u201cWhat are the conclusions of the Warren Report?\u201d. The recursive, map-reduce like algorithm it employs allows it to build up the correct answer looking at all matched chunks of text from the Warren Commission Report and the responses from GPT-4 to each of them. This is not to say that tree_summarize is the only Response Mode you need when building a RAG-style application, however for the purposes of summarizing content from across a large body of text, it clearly has structural advantages that lend itself more effective than the other modes. However, it\u2019s important to understand the different motivations behind each of the other 4 Response Modes and to know which circumstances each might be the best tool for the job. With the rapidly increasing sizes of input token limits for newer LLM models, it can be argued that the need for RAG will slowly diminish. However, even in a world where LLMs are able to accept million token inputs, it will always behoove a consumer of an LLM to maximize the efficiency of the each token used in the context passed in each LLM call. As such, the LlamaIndex framework provides a very neat and consistent programming model to build RAG-style applications, certainly more so than LangChain. For those of you interested in learning more about LlamaIndex and building with it, I encourage you to visit the LlamaIndex documentation , as the team has done an excellent job of building a set of , clear and easy to work through tutorials that outline the many capabilities of LlamaIndex. Bobby Gill is the co-founder and Chief Architect of BlueLabel, an award winning digital product agency headquartered in New York. With over two decades of experience in software development, he is a seasoned full-stack engineer, software architect, and AI practitioner. Bobby currently leads the BlueLabel AI/ML practice, where he is leading a team of engineers operationalizing the transformational capabilities of generative AI within BlueLabel and for a number of enterprise clients. A Digital Transformation Agency. 4.8/5 Overall Rating \u00a9 2024 BlueLabel | All Rights Reserved \u2013 Total Rating 4.8 out of 5 based on 40+ reviews", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "340fc66e-460e-4893-b469-dfa1ee1209d8": {"__data__": {"id_": "340fc66e-460e-4893-b469-dfa1ee1209d8", "embedding": null, "metadata": {"tag": "p", "url": "https://medium.com/@aneesha161994/part-2-llama-index-question-answering-in-rag-b174fd05c371", "category": "history"}, "excluded_embed_metadata_keys": ["url"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec565d57-00c2-4ac8-aead-fc676f8b2f1d", "node_type": "4", "metadata": {"url": "https://medium.com/@aneesha161994/part-2-llama-index-question-answering-in-rag-b174fd05c371", "category": "history"}, "hash": "ae665e2d005bbdfff3b375a6914e1973dee4b858e1720088d6b43fa44994c943", "class_name": "RelatedNodeInfo"}}, "text": "Sign up Sign in Sign up Sign in Aneesha B Soman Follow -- 1 Listen Share In our previous article we had seen the first 3 stages. You can find the article here: https://medium.com/@aneesha161994/question-answering-in-rag-using-llama-index-92cfc0b4dae3 For RAG question answering we need to use Query Engine. Hence, we perform Querying using a Query Engine. We already have performed Indexing and stored our documents. Now think of a query engine as a smart assistant you can ask questions to about your data stored. Just like how you might ask a teacher a question in class, you can ask the query engine questions about your information. Here\u2019s how it works: 1.Knowledge Base : The knowledge base is like a library filled with useful information such as FAQs, manuals, and other relevant documents. When a question is asked, this is where the system looks to find the answer. 2.Input : You give the query engine a question in regular language. 3. Processing : The query engine takes your question and figures out the best way to find the answer. It might look through different indexes or sources of information to get the right response. 4.Output : After doing its work, the query engine gives you a detailed answer. 5.Composition : You can even combine multiple query engines together to do more complex tasks. It\u2019s like having different experts working together to solve a problem. When we consider querying, there are various aspects: This is the pipeline of a Query Engine. We will understand each of these in detail. Imagine you\u2019re looking for something specific in a big library. Step 1: Build Index We build Index of the books in the Library, which will have the Document details. Here we have Document 1 with Cat details and Document 2 with Chair details. Step 2 Retriever: i. A retriever is like a librarian who helps you find the right books. Our storage are the Books. ii.Here\u2019s how it works: a. User Query: You ask the retriever a question or send a message, just like you would ask a librarian for help finding a book. b. Retriever: The retriever then searches through its resources to find the most relevant information based on your question . It\u2019s like the librarian searching through the library\u2019s catalog to find books related to your topic. The Llama index uses VectorIndexRetriever for performing this retrieval. This retrieves the nodes that most closely match our query in similarity. iii. This output for the above query will be: cat iv. The cat document will have nodes which might look like this Now we need to preprocess these nodes Step 3:Node Postprocessors Any postprocessing steps are applied to the retrieved text chunks. For example, if there\u2019s a postprocessor that filters out irrelevant information or reorders the chunks for better coherence, it\u2019s applied at this stage. Step 4: Response Synthesizer This is where the magic happens. The Response Synthesizer takes the filtered and processed nodes and generates a coherent response for the user query. It could be as simple as concatenating the book titles and authors into a single response or as complex as organizing them into categories or building a hierarchical structure based on genres. Let\u2019s say the user query \u201cscience fiction books\u201d retrieves the following nodes: Now, the Response Synthesizer could generate a response like: \u201cHere are some science fiction books you might enjoy: Step 1 Build Index: a. We build Index of the books in the Library, which will have the Document details. Here we have Document 1 with Cat details and Document 2 with Chair details. b. We index and store the data, we had understood this in our previous article. Lets look at a quick code on how to do this: A: Import OpenAI details B: Create document or node C: Create Index using Transformation Step 4: Now we would not want to store the index values. This is done in this example using local storage. Incase there is previous storage available then we fetch the index Now we have the index and storage. Here the index will have text of document, metadata and relationship. The topics we would discuss under the Retriever are: A. Definition of Retriever as per documentation B. Retrieval Mechanisms: Text and Document Based C. Advantages of using Document retrieval with summaries over top-k over a text-chunk D. VectorIndexRetriever Class E. Retrieval Types Retrievers are responsible for fetching the most relevant context given a user query (or chat message).It can be built on top of indexes, but can also be defined independently. It is used as a key building block in query engines (and Chat Engines) for retrieving relevant context. There are different types of retrieval mechanisms when we send in a query: In this article we will look into Text based and Document Summary Based Retrieval Mechanisms. a. Index methodolgy: When we have a text we use the text based methodology to extract/index the text into a structured format . This is done using VectorStoreIndex b. What all contains in index : Here we have the text split and stored as nodes, which will have metadata + relationship also being stored. c. Storage : Then these nodes alone are embedded and stored in a vector store. d. Retrieval method : Now when we query, we have embedding-based retrieval over text chunks. The retrieval is done such that it returns relevant chunks at the node-level. a. Index methodolgy: Here we will extract/index the text into an unstructured text summary for each document . This is done through Document Summary Index b. What all contains in index : We index more information than a single text chunk, and carries more semantic meaning than keyword tags. c. Storage: Both the summary and the nodes are stored within our Document Store abstraction. We would need to map from the summary to the source document/nodes. d. Retrieval method: At query-time, we employ distinct approaches to retrieve relevant documents based on their summaries: While LLM-based retrieval offers significant advantages, there are several important limitations and considerations, particularly in this initial version of Llama Index: There are various problems of text-chunk such as: The document retrieval with summaries method solves these problems. It gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it\u2019s also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags! When the query is \u201cA black cat\u201d to the retriever, it gives back from the document the closest 1relation which was cat .(The document had Dog and Cat) The VectorIndexRetriever has a fuction, retrieve() which will return a list of nodes for a given query a.What are the retrieval types? For this article we would dwell into Index retrievers. b. How to configure the Index retriever? When we want to configure a Index retriever, we configure the retriever using the RetrieverMode. We use this Retriever Mode to get a retriever from any given index. Depending on the index type , we choose a different method for selecting nodes using the \u201cRetrieverMode\u201d option. c. Retriever mode types: i. Summary Index: It will have a retriever modes to be default, embedding, llm. If the retriever mode is LLM, it means the Example: This creates a SummaryIndexLLMRetriever class on top of the summary index. You can access the details of how to use this class from here: https://ts.llamaindex.ai/api/classes/SummaryIndexLLMRetriever Now if we need to use this, we create an object from this class and use it, lets see this in example: Similar to the above we can look into the documentation and create our retriever using the retriever model for the other Index. ii. Tree Index: It will have a retriever modes to be select_leaf, select_leaf_embedding, all_leaf and root. iii. Keyword Table Index: It will have a retriever modes to be default,simple and rake iv. Knowledge Graph Index: It will have a retriever modes to be keyword,embedding and hybrid v. Document Summary Index: It will have a retriever modes to be llm and embedding. Now the nodes created through these retriever modes are sent into the Node Postporcessor Node postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them. a. Similarity Postprocessor class for filtering Nodes: This class helps in filtering both nodes created as well as retrieved nodes: 1.Nodes created: 2.Filtering Retrieved nodes: b. Time Weighted Postprocessor class With Query Engine: c. Other Classes: The postprocess_nodes are present in all the classes which are called. d. Custom Classes created by us: The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM. It generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a Response object. There are different modes of working for the response synthesizer a. It Refines and improves an answer by taking one section of text at a time . b. Lets take an example and understand step by step: a. It Queries an LLM with the same prompt across multiple text chunks, and return a formatted list of responses b. Lets understand with an example: The same as Refine, but puts as much text as possible into each LLM call The same as Accumulate , but puts as much text as possible a. Creates a bottom-up summary from the provided text chunks, and return the root summary b. How does it work? 1.User Query: The user inputs the query \u201cscience fiction books.\u201d 2.Text Chunks: The system divides the query into text chunks, which could include \u201cscience,\u201d \u201cfiction,\u201d and \u201cbooks.\u201d 3.Bottom-Up Summary: Instead of querying the LLM directly, the system constructs a hierarchical tree structure based on the text chunks. Each text chunk serves as a node in the tree, with relationships indicating their hierarchical organization. For instance: In this tree structure, \u201cscience\u201d is the parent node of \u201cfiction,\u201d and \u201cfiction\u201d is the parent node of \u201cbooks.\u201d This represents the hierarchical relationship between the concepts in the user query. Summary Generation : The system generates summaries for each level of the tree, starting from the bottom and moving upwards. It synthesizes information related to each concept at each level of the hierarchy. 4. Root Summary : After generating summaries for each level of the hierarchy, the system combines them into a root summary. This root summary encapsulates the key insights and information derived from the bottom-up summarization process. Root Summary example: \u201cExplore the intersection of science and fiction through a diverse collection of science fiction books. Dive into captivating narratives that blend scientific concepts with imaginative storytelling.\u201d a.Combine and truncate all text chunks, and summarize in a single LLM call b. Heres how it works: Example of Final Summary: \u201cDiscover a captivating collection of science fiction books that explore imaginative worlds, futuristic technologies, and thought-provoking concepts. Dive into thrilling adventures and explore the boundaries of science and fiction.\u201d Now that we have given the parameters and we have created a response_synthesizer, lets configure it along with retriever and node postprocessor. 1.The RetrieverQueryEngine combines the Retriever and the Response Synthesizer. 2.It uses a retriever, like the VectorStoreRetriever, to fetch relevant IndexNode objects from the VectorStoreIndex based on the user query. The retriever scans through the index and identifies documents (represented as vectors) that are most similar to the user\u2019s query. Once the retriever retrieves relevant documents from the index, the RetrieverQueryEngine uses a response synthesizer to generate a natural language response based on these retrieved documents and the user query. The response synthesizer could employ various techniques to summarize, organize, or present the retrieved information in a coherent and user-friendly manner. 3. Query Engines examples Example 1 Vector Query Engine: User Query: A user inputs a query, such as \u201cscience fiction books.\u201d In this query \u201cscience fiction books,\u201d the RetrieverQueryEngine would use the VectorStoreRetriever to find documents in the index that are most relevant to science fiction books. These documents might include titles, summaries, or other metadata related to science fiction literature. Then, the response synthesizer would generate a response, such as a list of recommended science fiction books or a summary of common themes in science fiction literature, based on the retrieved documents and the user query. Example 2 Keyword Query Engine: Example 3. Custom create the retriever with these RetrieverQueryEngine: Once the Query engine is created, the query function is called with the user query sent in as the input. We obtain the response as output. Do Subscribe to get more of such content! Happy coding :) -- -- 1 An AI Engineer with a passion for NLP. A Guitarist, Singer, Sketch artist and Tennis player as well. Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1131e3cd-ef83-4a6d-8acf-cd95b1a96b61": {"__data__": {"id_": "1131e3cd-ef83-4a6d-8acf-cd95b1a96b61", "embedding": null, "metadata": {"category": "note"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/", "node_type": "4", "metadata": {"category": "note"}, "hash": "aefd5577b2288e9a45668bc0d94c257fdd728c91bfa621d97bdf495ac712cc1a", "class_name": "RelatedNodeInfo"}}, "text": "The concept of Retrieval Augmented Generation (RAG) describes an approach to allow an LLM to answer questions based on data that it wasn\u2019t originally trained on. Instead, with the RAG-approach, a query to an LLM is broken into two parts: a \u2018retrieval step\u2019 and then a \u2018generation step\u2019. Essentially, RAG is a mechanism to work within the input size restrictions of LLMs by only including in the prompt the most relevant parts of the dataset needed to answer a query.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"929d3638-607b-4906-92ed-38af34a5c687": {"doc_hash": "9b9f7b7050f404e355df94e8c19ee247acb54ec50f125e30c4cef1aa773219e1", "ref_doc_id": "903007c0-81dc-4286-bafe-e76a64777a77"}, "340fc66e-460e-4893-b469-dfa1ee1209d8": {"doc_hash": "a60e9b6a8ec1eeea614469163ccaff52825763523f5d369e79a949f4a23e78d0", "ref_doc_id": "ec565d57-00c2-4ac8-aead-fc676f8b2f1d"}, "1131e3cd-ef83-4a6d-8acf-cd95b1a96b61": {"doc_hash": "aefd5577b2288e9a45668bc0d94c257fdd728c91bfa621d97bdf495ac712cc1a", "ref_doc_id": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}, "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/": {"doc_hash": "aefd5577b2288e9a45668bc0d94c257fdd728c91bfa621d97bdf495ac712cc1a"}}, "docstore/ref_doc_info": {"903007c0-81dc-4286-bafe-e76a64777a77": {"node_ids": ["929d3638-607b-4906-92ed-38af34a5c687"], "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/", "category": "history"}}, "ec565d57-00c2-4ac8-aead-fc676f8b2f1d": {"node_ids": ["340fc66e-460e-4893-b469-dfa1ee1209d8"], "metadata": {"tag": "p", "url": "https://medium.com/@aneesha161994/part-2-llama-index-question-answering-in-rag-b174fd05c371", "category": "history"}}, "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/": {"node_ids": ["1131e3cd-ef83-4a6d-8acf-cd95b1a96b61"], "metadata": {"category": "note"}}}}