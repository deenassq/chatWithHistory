{"docstore/data": {"4c692992-7ffa-4c8d-8d7e-7fc9ab0269d6": {"__data__": {"id_": "4c692992-7ffa-4c8d-8d7e-7fc9ab0269d6", "embedding": null, "metadata": {"tag": "p", "url": "https://app.slack.com/client/T06PYF8R5J7/C079BRFKJJW"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34948ca5-f1a0-4b26-bc2b-89e48e8baef4", "node_type": "4", "metadata": {"url": "https://app.slack.com/client/T06PYF8R5J7/C079BRFKJJW"}, "hash": "a1ea7f1592f641d79a2d60fbb40041561e3463ddcd3a73ec35d005d5f235697b", "class_name": "RelatedNodeInfo"}}, "text": "We're quite sorry about this! Before you try to troubleshoot, please do check https://slack-status.com - the problem may be on our end. Here are a few things to try: Check our Help Center for more details, or drop us a line .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7dc88c2-0a49-4d20-bdc6-ab70a6e9a732": {"__data__": {"id_": "d7dc88c2-0a49-4d20-bdc6-ab70a6e9a732", "embedding": null, "metadata": {"tag": "p", "url": "https://aws.amazon.com/what-is/reinforcement-learning/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a273f6c-7ee2-47b2-9a33-efe902abd2a2", "node_type": "4", "metadata": {"url": "https://aws.amazon.com/what-is/reinforcement-learning/"}, "hash": "ccd0cfd61797360baaf358ef00248e21d734c91045e882f820391be6c5e5f879", "class_name": "RelatedNodeInfo"}}, "text": "We use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can click \u201cCustomize cookies\u201d to decline performance cookies. If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To continue without accepting these cookies, click \u201cContinue without accepting.\u201d To make more detailed choices or learn more, click \u201cCustomize cookies.\u201d Essential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. Performance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes. Functional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly. Advertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising. Blocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by clicking Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the AWS Cookie Notice. We will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists. Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored. RL algorithms use a reward-and-punishment paradigm as they process data. They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes. The algorithms are also capable of delayed gratification. The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments. There are many benefits to using reinforcement learning (RL). However, these three often stand out. RL algorithms can be used in complex environments with many rules and dependencies. In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment. Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results. In traditional ML algorithms, humans must label data pairs to direct the algorithm. When you use an RL algorithm, this isn\u2019t necessary. It learns by itself. At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections. RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences. It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards. For example, decisions about energy consumption or storage might have long-term consequences. RL can be used to optimize long-term energy efficiency and cost. With appropriate architectures, RL agents can also generalize their learned strategies across similar but not identical tasks. Reinforcement learning (RL) can be applied to a wide range of real-world use cases. We give some examples next. In applications like recommendation systems, RL can customize suggestions to individual users based on their interactions. This leads to more personalized experiences. For example, an application may display ads to a user based on some demographic information. With each ad interaction, the application learns which ads to display to the user to optimize product sales. Traditional optimization methods solve problems by evaluating and comparing possible solutions based on certain criteria. In contrast, RL introduces learning from interactions to find the best or close-to-best solutions over time. For example, a cloud spend optimizing system uses RL to adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations. It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization. The dynamics of financial markets are complex, with statistical properties that change over time. RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts. For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards. It dynamically creates a value function and develops a strategy to maximize profits. The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology. For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell. Soon, the child learns which combination of activities results in the end reward. An RL algorithm mimics a similar learning process. It tries different activities to learn the associated negative and positive values to achieve the end reward outcome. In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps. At every step, the agent takes a new action that results in a new environment state. Similarly, the current state is attributed to the sequence of previous actions. Through trial and error in moving through the environment, the agent builds a set of if-then rules or policies. The policies help it decide which action to take next for optimal cumulative reward. The agent must also choose between further environment exploration to learn new state-action rewards or select known high-reward actions from a given state. This is called the exploration-exploitation trade-off . There are various algorithms used in reinforcement learning (RL)\u2014such as Q-learning, policy gradient methods, Monte Carlo methods, and temporal difference learning. Deep RL is the application of deep neural networks to reinforcement learning. One example of a deep RL algorithm is Trust Region Policy Optimization (TRPO). All these algorithms can be grouped into two broad categories. Model-based RL is typically used when environments are well-defined and unchanging and where real-world environment testing is difficult. The agent first builds an internal representation (model) of the environment. It uses this process to build this model: Once the model is complete, the agent simulates action sequences based on the probability of optimal cumulative rewards. It then further assigns values to the action sequences themselves. The agent thus develops different strategies within the environment to achieve the desired end goal. Consider a robot learning to navigate a new building to reach a specific room. Initially, the robot explores freely and builds an internal model (or map) of the building. For instance, it might learn that it encounters an elevator after moving forward 10 meters from the main entrance. Once it builds the map, it can build a series of shortest-path sequences between different locations it visits frequently in the building. Model-free RL is best to use when the environment is large, complex, and not easily describable. It\u2019s also ideal when the environment is unknown and changing, and environment-based testing does not come with significant downsides. The agent doesn\u2019t build an internal model of the environment and its dynamics. Instead, it uses a trial-and-error approach within the environment. It scores and notes state-action pairs\u2014and sequences of state-action pairs\u2014to develop a policy. Consider a self-driving car that needs to navigate city traffic. Roads, traffic patterns, pedestrian behavior, and countless other factors can make the environment highly dynamic and complex. AI teams train the vehicle in a simulated environment in the initial stages. The vehicle takes actions based on its current state and receives rewards or penalties. Over time, by driving millions of miles in different virtual scenarios, the vehicle learns which actions are best for each state without explicitly modeling the entire traffic dynamics. When introduced in the real world, the vehicle uses the learned policy but continues to refine it with new data. While supervised learning, unsupervised learning, and reinforcement learning (RL) are all ML algorithms in the field of AI, there are distinctions between the three. Read about supervised and unsupervised learning \u00bb In supervised learning, you define both the input and the expected associated output. For instance, you can provide a set of images labeled dogs or cats, and the algorithm is then expected to identify a new animal image as a dog or cat. Supervised learning algorithms learn patterns and relationships between the input and output pairs. Then, they predict outcomes based on new input data. It requires a supervisor, typically a human, to label each data record in a training data set with an output. In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance. During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes. By rewarding desired behaviors, you give weightage to the best outcomes. Unsupervised learning algorithms receive inputs with no specified outputs during the training process. They find hidden patterns and relationships within the data using statistical means. For instance, you could provide a set of documents, and the algorithm may group them into categories it identifies based on the words in the text. You do not get any specific outcomes; they fall within a range. Conversely, RL has a predetermined end goal. While it takes an exploratory approach, the explorations are continuously validated and improved to increase the probability of reaching the end goal. It can teach itself to reach very specific outcomes. While reinforcement learning (RL) applications can potentially change the world, it may not be easy to deploy these algorithms. Experimenting with real-world reward and punishment systems may not be practical. For instance, testing a drone in the real world without testing in a simulator first would lead to significant numbers of broken aircraft. Real-world environments change often, significantly, and with limited warning. It can make it harder for the algorithm to be effective in practice. Like any field of science, data science also looks at conclusive research and findings to establish standards and procedures. Data scientists prefer knowing how a specific conclusion was reached for provability and replication. With complex RL algorithms, the reasons why a particular sequence of steps was taken may be difficult to ascertain. Which actions in a sequence were the ones that led to the optimal end result? This can be difficult to deduce, which causes implementation challenges. Amazon Web Services (AWS) has many offerings that help you develop, train, and deploy reinforcement learning (RL) algorithms for real-world applications. With Amazon SageMaker , developers and data scientists can quickly and easily develop scalable RL models. Combine a deep learning framework (like TensorFlow or Apache MXNet), an RL toolkit (like RL Coach or RLlib), and an environment to mimic a real-world scenario. You can use it to create and test your model. With AWS RoboMaker , developers can run, scale, and automate simulation with RL algorithms for robotics without any infrastructure requirements. Get hands-on experience with AWS DeepRacer , the fully autonomous 1/18th scale race car. It boasts a fully configured cloud environment that you can use to train your RL models and neural network configurations. Get started with reinforcement learning on AWS by creating an account today.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aed79758-aaa9-438f-b3cf-c8e396aacbc5": {"__data__": {"id_": "aed79758-aaa9-438f-b3cf-c8e396aacbc5", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/api_reference/indices/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8058805-f4bb-4092-8b01-a7d8d1e142a2", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/api_reference/indices/"}, "hash": "7187499ccbe4edd2c88177e6897a5e68fe7ab7885dbffabee4dbaad8f4c99724", "class_name": "RelatedNodeInfo"}}, "text": "Base index classes. Bases: Generic[IS] , ABC Base LlamaIndex. Parameters: List of nodes to index Whether to show tqdm progress bars. Defaults to False. Service context container (contains components like LLM, Embeddings, etc.). Get the index struct. Get the index struct. Get the docstore corresponding to the index. Retrieve a dict mapping of ingested documents and their nodes+metadata. Create index from documents. Parameters: List of documents to build the index from. Set the index id. NOTE: if you decide to set the index_id on the index_struct manually, you will need to explicitly call add_index_struct on the index_store to update the index store. Parameters: Index id to set. Build the index from nodes. Insert nodes. Insert a document. Delete a list of nodes from the index. Parameters: A list of doc_ids from the nodes to delete Delete a document from the index. All nodes in the index related to the index will be deleted. Parameters: A doc_id of the ingested document Delete a document and it's nodes by using ref_doc_id. Update a document and it's corresponding nodes. This is equivalent to deleting the document and then inserting it again. Parameters: document to update kwargs to pass to insert kwargs to pass to delete Update a document and it's corresponding nodes. This is equivalent to deleting the document and then inserting it again. Parameters: document to update kwargs to pass to insert kwargs to pass to delete Refresh an index with documents that have changed. This allows users to save LLM and Embedding model calls, while only updating documents that have any changes in text or metadata. It will also insert any documents that previously were not stored. Refresh an index with documents that have changed. This allows users to save LLM and Embedding model calls, while only updating documents that have any changes in text or metadata. It will also insert any documents that previously were not stored. Convert the index to a query engine. Calls index.as_retriever(**kwargs) to get the retriever and then wraps it in a RetrieverQueryEngine.from_args(retriever, **kwrags) call. Convert the index to a chat engine. Calls index.as_query_engine(llm=llm, **kwargs) to get the query engine and then wraps it in a chat engine based on the chat mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1db2cdcc-f091-4b91-8ef6-65ae162b47a8": {"__data__": {"id_": "1db2cdcc-f091-4b91-8ef6-65ae162b47a8", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/indexing/document_management/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9d4219d-8651-4813-a71c-c37b43df0e55", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/module_guides/indexing/document_management/"}, "hash": "9c8eea65f87b08156dc693cccbf6d178fac798c6b4dd990107b578bb0fec9769", "class_name": "RelatedNodeInfo"}}, "text": "Most LlamaIndex index structures allow for insertion , deletion , update , and refresh operations. You can \"insert\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index. The underlying mechanism behind insertion depends on the index structure. For instance, for the summary index, a new Document is inserted as additional node(s) in the list. For the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store. An example notebook showcasing our insert capabilities is given here . In this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures. An example code snippet is given below: You can \"delete\" a Document from most index data structures by specifying a document_id. ( NOTE : the tree index currently does not support deletion). All nodes corresponding to the document will be deleted. delete_from_docstore will default to False in case you are sharing nodes between indexes using the same docstore. However, these nodes will not be used when querying when this is set to False as they will be deleted from the index_struct of the index, which keeps track of which nodes can be used for querying. If a Document is already present within an index, you can \"update\" a Document with the same doc id_ (for instance, if the information in the Document has changed). Here, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional. If you set the doc id_ of each document when loading your data, you can also automatically refresh the index. The refresh() function will only update documents who have the same doc id_ , but different text contents. Any documents not present in the index at all will also be inserted. refresh() also returns a boolean list, indicating which documents in the input have been refreshed in the index. Again, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional. If you print() the output of refresh() , you would see which input documents were refreshed: This is most useful when you are reading from a directory that is constantly updating with new information. To automatically set the doc id_ when using the SimpleDirectoryReader , you can set the filename_as_id flag. You can learn more about customzing Documents . Any index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore. Each entry in the output shows the ingested doc id_ s as keys, and their associated node_ids of the nodes they were split into. Lastly, the original metadata dictionary of each input document is also tracked. You can read more about the metadata attribute in Customizing Documents .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2799b79f-14dc-4799-b496-f7d7ca30209a": {"__data__": {"id_": "2799b79f-14dc-4799-b496-f7d7ca30209a", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba773043-68e9-41bb-bf37-8468e78e61fa", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/"}, "hash": "68fdeedb2de19e0306d97aeb690a10c34fd9ea6968a3af227c2f75e2c8ddb860", "class_name": "RelatedNodeInfo"}}, "text": "Document and Node objects are core abstractions within LlamaIndex. A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below. Note : We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities. A Node represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes. Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our NodeParser classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \"file_name\" filed in the Document is propagated to every Node). Here are some simple snippets to get started with Documents and Nodes. Take a look at our in-depth guides for more details on how to use Documents/Nodes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab260ffc-9379-4126-8deb-05b4d8d0a6f1": {"__data__": {"id_": "ab260ffc-9379-4126-8deb-05b4d8d0a6f1", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4edef67d-2227-43fc-b8b4-618a2b53af52", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/"}, "hash": "166c395dbf6140ec6d3b6fbe74e0a895456587cd9139de35bdb7ef2cfa8606f1", "class_name": "RelatedNodeInfo"}}, "text": "Nodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures. Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our NodeParser classes. For instance, you can do You can also choose to construct Node objects manually and skip the first section. For instance, The RelatedNodeInfo class can also store additional metadata if needed: Each node has an node_id property that is automatically generated if not manually specified. This ID can be used for a variety of purposes; this includes being able to update nodes in storage, being able to define relationships between nodes (through IndexNode ), and more. You can also get and set the node_id of any TextNode directly. Hi, how can I help you? \ud83e\udd99", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6cd537c-1ced-4d67-bec6-5d90bbebb76c": {"__data__": {"id_": "c6cd537c-1ced-4d67-bec6-5d90bbebb76c", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/understanding/querying/querying/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10084969-a2c5-4bb5-b7d1-d8a08b05d58b", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/understanding/querying/querying/"}, "hash": "14697cdd531ba0e0cc5268a8ba37b5d48d4a78647241334805710d7a331a8a63", "class_name": "RelatedNodeInfo"}}, "text": "Now you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying. At its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction. More complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components. The basis of all querying is the QueryEngine . The simplest way to get a QueryEngine is to get your index to create one for you, like this: However, there is more to querying than initially meets the eye. Querying consists of three distinct stages: Tip You can find out about how to attach metadata to documents and nodes . LlamaIndex features a low-level composition API that gives you granular control over your querying. In this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant. You can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces. For a full list of implemented components and the supported configurations, check out our reference docs . Let's go into more detail about customizing each step: There are a huge variety of retrievers that you can learn about in our module guide on retrievers . We support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality. For example: The full list of node postprocessors is documented in the Node Postprocessor Reference . To configure the desired node postprocessors: After a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information. You can configure it via Right now, we support the following options: You may want to ensure your output is structured. See our Query Engines + Pydantic Outputs to see how to extract a Pydantic object from a query engine class. Also make sure to check out our entire Structured Outputs guide. If you want to design complex query flows, you can compose your own query pipeline across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components. Take a look at our Query Pipelines Module Guide for more details.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87fba420-19be-4d5b-ab4b-4622d42b6557": {"__data__": {"id_": "87fba420-19be-4d5b-ab4b-4622d42b6557", "embedding": null, "metadata": {"tag": "p", "url": "https://drive.google.com/file/u/0/d/1SPS5LeqXBBKmyQGIR2lrpeagLWwVerpU/edit"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a434e7a-9abb-4432-bc6a-8621b18e9c86", "node_type": "4", "metadata": {"url": "https://drive.google.com/file/u/0/d/1SPS5LeqXBBKmyQGIR2lrpeagLWwVerpU/edit"}, "hash": "0b222c9350ae12a84359a8974c5fa8a15360e31bd96d2007f8ca15f8a90dec99", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4bf41c0-b3f7-4fbb-9105-4bd7e1126f62": {"__data__": {"id_": "c4bf41c0-b3f7-4fbb-9105-4bd7e1126f62", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c89ded7b-5760-4b9d-ad7f-a557beed000a", "node_type": "4", "metadata": {"url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}, "hash": "1409b5dc03e945c26ac3bacee2d579b0ba77c9ea48ab199d3baff71ca5f35997", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.\ud83d\ude80 Welcome to our AI-Generated Text Detection project! In this repository, we present a robust solution for detecting AI-generated text using BERT, a cutting-edge natural language processing model. Whether you're a researcher, developer, or a curious enthusiast, this project empowers you to explore, understand, and combat AI-generated content effectively. AI-generated content is becoming increasingly sophisticated, making it challenging to distinguish between genuine and computer-generated text. Our project aims to tackle this issue by leveraging the power of BERT (Bidirectional Encoder Representations from Transformers) to identify and flag AI-generated text segments. Whether you're dealing with chatbots, articles, or social media posts, our solution offers accurate detection, ensuring the authenticity of digital content. Follow these simple steps to get started with our AI-Generated Text Detection tool: Our solution follows a comprehensive approach to AI-generated text detection: Data Preprocessing: We clean and preprocess the textual data, removing noise and irrelevant information to enhance the accuracy of our model. BERT Tokenization: Leveraging the BERT tokenizer, we encode the preprocessed text, preparing it for input into our detection model. Model Training: Using a BERT-based sequence classification model, we train the system to distinguish between genuine and AI-generated text with a high degree of accuracy. Predictions: Once trained, the model generates predictions for test data, highlighting potential AI-generated content segments. Result Analysis: The results are saved in a CSV file, allowing users to review and analyze the detected segments along with their confidence scores. We welcome contributions from the community! Whether you're a seasoned developer, a data science enthusiast, or a domain expert, your insights and expertise can enhance our project. \ud83d\ude80 Connect With Me: If you find this project interesting or helpful, don't hesitate to follow me for more exciting updates and projects! Let's learn and grow together! \ud83c\udf1f AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.\ud83d\ude80", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad8a32fb-8eac-4e0d-838b-47210d45831d": {"__data__": {"id_": "ad8a32fb-8eac-4e0d-838b-47210d45831d", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/deenassq/Sentiment-Analysis-for-STC-Customers-Tweets"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3d02fa5-7dec-4b91-9e79-d261e0322194", "node_type": "4", "metadata": {"url": "https://github.com/deenassq/Sentiment-Analysis-for-STC-Customers-Tweets"}, "hash": "4ab8ea7458144e5f05c0d0b0b81140cac431cda2be545c482645a45f010f6e9a", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This project is Machine-Learning based using Natural Language Processing techniques. Our project aims to determine the degree of STC's customers experience based on their tweets about STC, both positively, negatively, and naturally. It serves Saudi Arabia and helps increase customer satisfaction. We used three ML classifiers, which are: DT (Decision Tree), MNB (Multinomial Naive Bayes), and SVM (Support Vector Machine).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95b002f8-bf16-40b2-8b77-ee96af602c73": {"__data__": {"id_": "95b002f8-bf16-40b2-8b77-ee96af602c73", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/kv-22"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "261dd891-15e7-47f7-9fe3-3d73179e2b35", "node_type": "4", "metadata": {"url": "https://github.com/kv-22"}, "hash": "e5ebd29fd7cf97c9b7a5a82f91d0345caf1e6228a9e5c65b47686f69c2c736c5", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . You can @mention other users and organizations to link to them. Prolog Python 1 Jupyter Notebook Forked from GDSC-IAU/Selection-Python Jupyter Notebook Forked from Joud73/Salary-Prediction Jupyter Notebook Java", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "218b28bb-9ae4-4cf9-a750-f88b188f4308": {"__data__": {"id_": "218b28bb-9ae4-4cf9-a750-f88b188f4308", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/kv-22/chatWithHistory"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ebfe72f-ec7b-4c88-bd2c-da17f5acf7f2", "node_type": "4", "metadata": {"url": "https://github.com/kv-22/chatWithHistory"}, "hash": "4102971b6fbbc03e17730d67e93a022d49088b758c20488ae79f90b7b689fdd2", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7b1b69a-ca6d-4ddd-bd61-3fc532ac311b": {"__data__": {"id_": "f7b1b69a-ca6d-4ddd-bd61-3fc532ac311b", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/kv-22/chatWithHistory/tree/main"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "063555a2-c6a8-4ba6-8d8b-d52137e29ace", "node_type": "4", "metadata": {"url": "https://github.com/kv-22/chatWithHistory/tree/main"}, "hash": "e38cc2b60431588737ab655303467c2fbc2ff914fba47c8e11712e3f4238915c", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "552bf27e-bce5-40bb-9d34-a3bede7a748b": {"__data__": {"id_": "552bf27e-bce5-40bb-9d34-a3bede7a748b", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/kv-22?tab=repositories"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b3511f5-b591-4d43-a74f-c21386aa80db", "node_type": "4", "metadata": {"url": "https://github.com/kv-22?tab=repositories"}, "hash": "999122ba9e19d89ba605b7871ef9dee61586297aee496deaf47154a138847cb0", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . You can @mention other users and organizations to link to them. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. first rep This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48a1e406-c8df-4700-bd1a-bd73a504c807": {"__data__": {"id_": "48a1e406-c8df-4700-bd1a-bd73a504c807", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/predlico/ARAGOG"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a47ab726-a49c-4ea5-925c-e1df812cd914", "node_type": "4", "metadata": {"url": "https://github.com/predlico/ARAGOG"}, "hash": "73c4f8ec117dd01bd30a0b568bffa32604845dbd5ff3b78442807389bd3bcc8a", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. ARAGOG- Advanced RAG Output Grading. Exploring and comparing various Retrieval-Augmented Generation (RAG) techniques on AI research papers dataset. Includes modular code for easy experimentation and reusability. This repository contains the code, data, and analysis for our study [link later] on advanced Retrieval-Augmented Generation (RAG) techniques. It's part of our scientific paper investigating the efficacy of various RAG techniques in enhancing the precision and contextual relevance of LLMs. To replicate our experiments or to analyze our results, please ensure to fill in the necessary API keys and other configurations by creating a .env file (see .sample.env ) - the .env is ignored in .gitignore for security. Setup the python environment using either venv or pyenv or your favourite python environment amanger. Call the environment aragog or anything you like. Then run pip install -r requirements.txt to install all necessary dependencies. The res_analysis.ipynb notebook provides a detailed examination of the experimental results stored in final_results.xlsx . To set up vector databases for experiments, run the vector_db.py script. Subsequently, execute main.py to perform the experiments. Post-experimentation, use res_analysis.ipynb for analyzing the results. Helper functions in utils.py are employed across scripts to streamline processes. Contributions are welcome. For any changes or enhancements, please open an issue first to discuss what you would like to change. This project is open-source and available under the MIT License . ARAGOG- Advanced RAG Output Grading. Exploring and comparing various Retrieval-Augmented Generation (RAG) techniques on AI research papers dataset. Includes modular code for easy experimentation and reusability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "482c8e4a-90fa-4d96-bf1f-2f212921eea5": {"__data__": {"id_": "482c8e4a-90fa-4d96-bf1f-2f212921eea5", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/run-llama/llama_index/issues/3928"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1b72519-0c6b-4d24-969e-83e11bb57508", "node_type": "4", "metadata": {"url": "https://github.com/run-llama/llama_index/issues/3928"}, "hash": "193ad052c8fe9333e386f4036d597ddaeae0b488d85d5ba24f80ee9b0cf8c23e", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Sorry, something went wrong. In a google drive folder I have 7 docs for which I have created the vector store index using GoogleDriveReader = download_loader('GoogleDriveReader') folder_id = '1LFa04mF3U300ttoej-EkWcT35sHHDZJL' loader = GoogleDriveReader() documents = loader.load_data(folder_id=folder_id) index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context ) Now in the same folder i have added some more documents , how I can update the index how can I use index.insert(document) how it will identify new documents only whether it will create index from scratch or it will append new index into that previously created files. please provide the details about this The text was updated successfully, but these errors were encountered: Sorry, something went wrong. I am currently working on the same problem, but i got the feeling the GPTVectorStoreIndex does not support updating or at least not inserting new documents. I also tryed the add and insert methods which are both do not exsist for the GPTVectorStoreIndex. Maybe we need to use another vector index... the documentation is not clear about that, so let me know if you find a solution. Here is my code WHICH DOES NOT WORK: AttributeError: 'GPTVectorStoreIndex' object has no attribute 'merge' AttributeError: 'GPTVectorStoreIndex' object has no attribute 'insert_documents' AttributeError: 'GPTVectorStoreIndex' object has no attribute 'add_documents'. Did you mean: 'from_documents'? Kind regards Update: Ive read somewhere setting overwrite=false should have this effect, however this also did not work. The index is overwritten... index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context, overwrite=False) Sorry, something went wrong. Sorry, something went wrong. I would love to have this feature added please. Sorry, something went wrong. Sorry, something went wrong. It already works actually So i check if the index exists (for example by checking if there are files... if not i create a new one, after saving it to disc i move the documents into another folder. If it exists i add the new documents to it and then move them into another folder after the index is updated. You can do this by adding new nodes to an existing index :) I hope this helps! So the magic happens with index.insert_nodes(new_nodes) Sorry, something went wrong. Sorry, something went wrong. Hey @j0schi , how did you check if there are files for index_exists variable ? Sorry, something went wrong. Sorry, something went wrong. Hi, this is probably not the best solution but it works for me :) You might have to change the folder / path according to your system. Sorry, something went wrong. Sorry, something went wrong. Thanks for your response! Actually I'm planning to use a vector store from those : https://gpt-index.readthedocs.io/en/latest/how_to/storage/customization.html#vector-store-integrations-and-storage instead of persist_dir Edit : I don't think that we can do that through llama_index, so I'll just use the library of the vector store Sorry, something went wrong. Sorry, something went wrong. Hey @j0schi , I don't know why but this code is not updating anything even code is almost same as yours. Sorry, something went wrong. Sorry, something went wrong. Take a look at http://localhost:8000/how_to/index/document_management.html , especially the refresh section! Sorry, something went wrong. Sorry, something went wrong. Its local server link can't check this? Sorry, something went wrong. Sorry, something went wrong. Its local server link can't check this? https://gpt-index.readthedocs.io/en/latest/how_to/index/document_management.html Sorry, something went wrong. Sorry, something went wrong. Have similar qustion : Do I need to persist my index again after inserting new data into my VectorStore Index or it it done automatically ? I am using MilvusVectorStore. My method : Sorry, something went wrong. Sorry, something went wrong. Have similar qustion : Do I need to persist my index again after inserting new data into my VectorStore Index or it it done automatically ? I am using MilvusVectorStore. My method : Based on my experience with FaissVectorStore , one doesn't have to persist and load the index again to make these newly added nodes available to the query. Sorry, something went wrong. Add your comment here... We don\u2019t support that file type. Try again with GIF, JPEG, JPG, MOV, MP4, PNG, SVG, WEBM, CPUPROFILE, CSV, DMP, DOCX, FODG, FODP, FODS, FODT, GZ, JSON, JSONC, LOG, MD, ODF, ODG, ODP, ODS, ODT, PATCH, PDF, PPTX, TGZ, TXT, XLS, XLSX or ZIP. Attaching documents requires write permission to this repository. Try again with GIF, JPEG, JPG, MOV, MP4, PNG, SVG, WEBM, CPUPROFILE, CSV, DMP, DOCX, FODG, FODP, FODS, FODT, GZ, JSON, JSONC, LOG, MD, ODF, ODG, ODP, ODS, ODT, PATCH, PDF, PPTX, TGZ, TXT, XLS, XLSX or ZIP. This file is empty. Try again with a file that\u2019s not empty. This file is hidden. Try again with another file. Something went really wrong, and we can\u2019t process that file. Try again. Nothing to preview No branches or pull requests Sorry, something went wrong and we weren't able to fetch your subscription status. Retry", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82ccf39b-6225-4a9e-8f1e-0040f45be498": {"__data__": {"id_": "82ccf39b-6225-4a9e-8f1e-0040f45be498", "embedding": null, "metadata": {"tag": "p", "url": "https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caa9fdad-b18c-478e-b162-b89e271ee36e", "node_type": "4", "metadata": {"url": "https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488"}, "hash": "b23ee262f633b0ddb22350d7af13ec1176ba762162cd337ca22af67f736e219d", "class_name": "RelatedNodeInfo"}}, "text": "Sign up Sign in Sign up Sign in shruti saxena Follow -- 40 Listen Share In this blog, I will focus on the challenges pertaining to model evaluation I came across while implementing a machine log analytics classification algorithm. Specifically, I will demonstrate the meaning of model evaluation metrics \u2014 precision and recall through real life examples, and explain the trade-offs involved. Though, my learnings are derived from my experience in the log analytics project, I will try to give generic examples to explain all the concepts. For the curious ones scratching their brains right now, here is a great reference paper to understand what log analytics is all about. For more details, please check out the references cited at the end of this blog. Before diving into the concept of precision and recall, let me recap for you what Type I and Type II errors signify. Type I and Type II Errors One fine morning, Jack got a phone call. It was a stranger on the line. Jack, still sipping his freshly brewed morning coffee, was barely in a position to understand what was coming for him. The stranger said, \u201cCongratulations Jack! You have won a lottery of $10 Million! I just need you to provide me your bank account details, and the money will be deposited in your bank account right way\u2026\u201d What are the odds of that happening? What should Jack do? What would you have done? Tricky, right? Let me try to explain the complexity here. Assuming Jack is a normal guy, he would think of this as a prank, or maybe, a scam to fetch his bank details, and hence will deny to provide any information. However, this decision is based on his assumption that the call was a hoax. If he is right, he will save the money in his bank account. But, if he is wrong, this decision would cost him a million dollars! Let\u2019s talk in statistical terms for a bit. According to me, the null hypothesis in this case is that this call is a hoax. As a matter of fact, if Jack would have believed the stranger and provided his bank details, and the call was in fact a hoax, he would have committed a type I error, also known as a false positive. On the other hand, had he ignored the stranger\u2019s request, but later found out that he actually had won the lottery and the call was not a hoax, he would have committed a Type II error, or a false negative. Now that we are clear with the concept of Type I and Type II errors, let us dive into the concept of precision and recall. Precision and Recall Often, we think that precision and recall both indicate accuracy of the model. While that is somewhat true, there is a deeper, distinct meaning of each of these terms. Precision means the percentage of your results which are relevant. On the other hand, recall refers to the percentage of total relevant results correctly classified by your algorithm. Undoubtedly, this is a hard concept to grasp in the first go. So, let me try to explain it with Jack\u2019s example. \u2026Feeling a bit panicky, Jack called up his bank to ensure his existing accounts were safe and all his credits were secure. After listening to Jack\u2019s story, the bank executive informed Jack that all his accounts were safe. However, in order to ensure that there is no future risk, the bank manager asked Jack to recall all instances in the last six months wherein he might have shared his account details with another person for any kind of transaction, or may have accessed his online account from a public system, etc\u2026 What are the chances that Jack will be able to recall all such instances precisely ? If you understood what I asked in the previous sentence with a cent per cent confidence, you have probably understood what recall and precision actually means. But, just to double check, here is my analysis. if Jack had let\u2019s say ten such instances in reality, and he narrated twenty instances to finally spell out the ten correct instances, then his recall will be a 100%, but his precision will only be 50%. Barring the time Jack spent on the phone call with the bank executive spelling out extra information, there was actually nothing much at stake here due to low precision. But, imagine if the same thing happens the next time you search for a product on let\u2019s say amazon. The moment you start getting irrelevant results, you would switch to another platform, or maybe even drop the idea of buying. This is the reason why both precision and recall are so important in your model. And by this time, you might have already guessed, one comes at the cost of another. Trade-off This is pretty intuitive. If you have to recall everything, you will have to keep generating results which are not accurate, hence lowering your precision. To exemplify this, imagine the case of digital world (again, amazon.com?), wherein there is a limited space on each webpage, and extremely limited attention span of the customer. Therefore, if the customer is shown a lot of irrelevant results and very few relevant results (in order to achieve a high recall), the customer will not keep browsing each and every product forever to finally find the one he or she intends to buy, and will probably switch to Facebook, twitter, or may be Airbnb to plan his or her next vacation. This is a huge loss, and hence the underlying model or algorithm would need a fix to balance the recall and precision. Similar thing happens when a model tries to maximize precision. Does a simpler metric exist? In most problems, you could either give a higher priority to maximizing precision, or recall, depending upon the problem you are trying to solve. But in general, there is a simpler metric which takes into account both precision and recall, and therefore, you can aim to maximize this number to make your model better. This metric is known as F1-score , which is simply the harmonic mean of precision and recall. To me, this metric seems much more easier and convenient to work with, as you only have to maximize one score, rather than balancing two separate scores. In fact, there are other ways to combine precision and recall into one score like a geometric mean of the two, and it might be worth exploring the different kinds and their respective trade-offs. So, what are the key takeaways? Precision and recall are two extremely important model evaluation metrics. While precision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by your algorithm. Unfortunately, it is not possible to maximize both these metrics at the same time, as one comes at the cost of another. For simplicity, there is another metric available, called F-1 score, which is a harmonic mean of precision and recall. For problems where both precision and recall are important, one can select a model which maximizes this F-1 score. For other problems, a trade-off is needed, and a decision has to be made whether to maximize precision, or recall. I hope that this blog was engaging and insightful. I look forward to your feedback in the comments section. And don\u2019t forget to read the reference articles, they are truly a wealth of knowledge. Happy reading! References Precision-Recall (scikit-learn) , The Relationship Between Precision-Recall and ROC Curves , A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation -- -- 40 Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2368581c-b2cc-4c46-8ef4-cf051fd7fcc5": {"__data__": {"id_": "2368581c-b2cc-4c46-8ef4-cf051fd7fcc5", "embedding": null, "metadata": {"tag": "p", "url": "https://repair2care.com/en_sa/services/exterior-repairs/alloy-wheels-repairs/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56cd6d27-ff55-45dd-96fb-73f90ff73476", "node_type": "4", "metadata": {"url": "https://repair2care.com/en_sa/services/exterior-repairs/alloy-wheels-repairs/"}, "hash": "d6efb574049e8fd770a6830fc332d41904515eefbeeb1d89e723f20dac4132fa", "class_name": "RelatedNodeInfo"}}, "text": "Sparen Sie 50 \u20ac bei Ihrer ersten Reparatur mit dem Code: Repair2Care Sparen Sie 50 \u20ac bei Ihrer ersten Reparatur mit dem Code: Repair2Care Have your wheels been damaged? And would you like to keep your original alloy wheels? Then we can help. We offer rim repair so you can keep your current rims for your car. We can repair your rims, whether you have curb damage, dents, scratches or scuffs. We specialize in alloy wheel repair and refurbishment and we make sure your wheels are as good as new - always at fixed and affordable prices. We take care of your damaged rims and make them look like new again. Our technicians have extensive experience in different types of wheel repairs - including straightening dented and warped wheels that may have been damaged in a car accident. We can also refurbish scratched rims by removing superficial damage, reconstructing the surface and then repainting the rim . The alloy wheel will then look like it did before it was damaged. With our diamond cut rim repair , we remove unwanted scratches and reconstruct the aluminum surface using a specially designed machine to make the rim look like new again. We allow you to keep your original rims, which is a great advantage as replacing them with new rims is often an expensive option. By keeping your original wheels, you also maintain the value of your car because you're not replacing the original wheels with a cheap pair of unoriginal ones. Our prices depend on the size and type of rim, as well as the type of repair. Here is a list of the types of rim repairs we offer and prices for our repairs: Your rims are exposed to daily wear and tear that can cause unsightly scratches or peeling paint, which can quickly give them a dull appearance. Luckily, these damages can be repaired and the result is wheels that look like new again. We use state-of-the-art, custom-designed equipment for our professional wheel restorations. Our specially designed wheel machines can both restore the shiny surface of Diamond Cut wheels and refinish alloy wheels that have been scratched or need a color change . In addition, we have specially designed machines that can straighten rims back to their original shape . Compared to the cost of buying new alloy wheels, repairing original alloy wheels is a significantly more economical solution. In most cases, you can save around 50-75% of the cost of original wheels, and when it comes to specialty wheels from selected car brands, the savings can be even greater. SAVE 50-75% ON THE PRICE OF ORIGINAL RIMS BY CHOOSING A REPAIR OVER A REPLACEMENT Repair2Care is first global workshop chain specializing exclusively in repairs of both interior and exterior cosmetic damage to the car as well as professional car care and protection services.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7ddc733-320f-40c7-a97a-0241900cd290": {"__data__": {"id_": "a7ddc733-320f-40c7-a97a-0241900cd290", "embedding": null, "metadata": {"tag": "p", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cf9fe29-5ba3-4efb-9fea-86a6edb4b770", "node_type": "4", "metadata": {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html"}, "hash": "fdc18b2c8fdad2b145121a30a8b5bb4aa49764e1f7fae7cb9b0bd0565d3b3e6b", "class_name": "RelatedNodeInfo"}}, "text": "Section Navigation Load and return the diabetes dataset (regression). Samples total 442 Dimensionality 10 Features real, -.2 < x < .2 Targets integer 25 - 346 Note The meaning of each feature (i.e. feature_names ) might be unclear (especially for ltg ) as the documentation of the original dataset is not explicit. We provide information that seems correct in regard with the scientific literature in this field of research. Read more in the User Guide . If True, returns (data, target) instead of a Bunch object. See below for more information about the data and target object. Added in version 0.18. If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric). The target is a pandas DataFrame or Series depending on the number of target columns. If return_X_y is True, then ( data , target ) will be pandas DataFrames or Series as described below. Added in version 0.23. If True, the feature variables are mean centered and scaled by the standard deviation times the square root of n_samples . If False, raw data is returned for the feature variables. Added in version 1.1. Dictionary-like object, with the following attributes. The data matrix. If as_frame=True , data will be a pandas DataFrame. The regression target. If as_frame=True , target will be a pandas Series. The names of the dataset columns. Only present when as_frame=True . DataFrame with data and target . Added in version 0.23. The full description of the dataset. The path to the location of the data. The path to the location of the target. Returns a tuple of two ndarray of shape (n_samples, n_features) A 2D array with each row representing one sample and each column representing the features and/or target of a given sample. Added in version 0.18. Examples Release Highlights for scikit-learn 1.2 Gradient Boosting regression Plot individual and voting regression predictions Model Complexity Influence Model-based and sequential feature selection Lasso and Elastic Net Lasso model selection via information criteria Lasso model selection: AIC-BIC / cross-validation Lasso path using LARS Linear Regression Example Sparsity Example: Fitting only features 1 and 2 Advanced Plotting With Partial Dependence Imputing missing values before building an estimator Plotting Cross-Validated Predictions Cross-validation on diabetes Dataset Exercise previous load_breast_cancer next load_digits \u00a9 Copyright 2007 - 2024, scikit-learn developers (BSD License).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31aaf241-ba89-460e-b8bf-6126f34d777b": {"__data__": {"id_": "31aaf241-ba89-460e-b8bf-6126f34d777b", "embedding": null, "metadata": {"tag": "p", "url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6715c28f-5680-4da9-8800-f98f728af043", "node_type": "4", "metadata": {"url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}, "hash": "f3f5a01b9e845ff0f65ac813f8b800690bdb661b957286ad148165d5da271f03", "class_name": "RelatedNodeInfo"}}, "text": "Have a recipe of your own to share? Submit your recipe There are a few crucial steps to baking the chewiest, tastiest chocolate chip cookies. First, skip using chips and opt for chunks. Second, instead of using just one type of chocolate (like semisweet), use a mix of semisweet, milk, and dark chocolate. Third, take the time to allow the cookie dough to rest overnight in the refrigerator. Yes, we know it's a pain, but doing so will yield a cookie with a more complex flavor and delicious toffee notes. Lastly, use an ice cream scoop to portion the cookie dough onto the baking sheet. This will produce even-sized cookies every single time. Follow these simple steps, and you'll be rewarded with a cookie that's crisp and chewy on the outside and gooey on the inside! 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min Inspired by buzzfeed.com 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min for 12 cookies Estimated values based on one serving size. Build your cart with Tasty, then choose how you want to get your order from Walmart. I love this recipe so much!! I am NEVER using chocolate chips ever again! These cookies are decadent, chewy, and oh so delicious! I used light brown sugar and salted butter. I highly recommend this recipe to any chocolate chip cookie lover! if the cookies get hard, store them in and airtight bag or container with a slice of bread and they will become soft again soon I have made these about 4 times now! My tip is to not melt the butter until it\u2019s completely liquid, then whisk it until the soft butter is incorporated with the fully melted butter before adding. Also if you let the dough rest in the fridge for 3 hours or more, they get nice and toffee-like. The cookies were so delicious \ud83d\ude0b They were a little bigger than expected. I put the cookies on a nonstick pan instead of parchment paper and they still turned out fine. \ud83d\ude0d They were really easy to make. I\u2019m 12 \ud83d\ude02 100% I\u2019d make again! I bake them for 5 minutes less to avoid over cooking as they\u2019re cooling. This ensures they will still be soft after even a week of storage (if they last that long). I\u2019ve made this recipe twice now and I changed the recipe a tad bit. I added 1/4 cup more flour, 1/2 teaspoon of salt and I used 3/4 cup of nestle\u2019s dark chocolate chips . I let them hang out in the fridge for about 2 hours. I also preheated my oven to 375 and let them bake for 10 mins and took them out to sit about 5 min to finish. Be sure to get the right kind of chocolate! Not chocolate chips. Get the chunks! Trust me, it makes a difference. When I was incorporating the melted butter and sugars, it doesn\u2019t look the same like the one in the video. I continued the recipe but it ended up looking like cake batter, so I added 1/4 cup of flour and it turned out great! I recommend mixing chocolate chips and chocolate chunks for my diversity Use half melted butter and half soften butter that\u2019s makes a chewier cookie Way too sweet, use less sugar These cookies are LEGIT the best cookies i\u2019ve ever made! Have made them several times and they come out great every time. I use a bit less brown sugar and a bit more white sugar so the cookie will spread. Really good! Inspired by buzzfeed.com", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a4bd6df-9c8e-4c9a-bc77-6ecaa7dc40a2": {"__data__": {"id_": "1a4bd6df-9c8e-4c9a-bc77-6ecaa7dc40a2", "embedding": null, "metadata": {"tag": "p", "url": "https://www.alloywheel.sa/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbb0e710-d7ac-48f0-b3ff-48a16b7e0cdf", "node_type": "4", "metadata": {"url": "https://www.alloywheel.sa/"}, "hash": "b22064aa24a9772b9eaa5e9340b4e48b89171d8485ba7e01107e5cc9a4bc1f46", "class_name": "RelatedNodeInfo"}}, "text": "Home Services About AWRS AlloyCatalog AlloyMagazine Reach Us More Give your Motorcycle that custom look! Get that custom look. HELMETS WHEELS BRAKE CALIPERS MOTORCYCLES When AWRS Saudi Arabia was founded in 2007, we set out to provide the most comprehensive wheel repairs and customization in the kingdom. Through periodic training of our staff by industry leaders around the world and facility upgrades with the latest technology, we have been able to do just that. In addition, we have integrated services outside the wheel industry including airbrushing, custom caliper painting, high temperature ceramic coating and \"Hyper Coating\". We understand how important your car is to you. We\u2019ve built an unparalleled foundation of knowledge through rigorous training. It is our standard for each new technician to shadow more experienced members and learn the tricks of the trade only the most seasoned techs know. If you are looking for the highest quality materials and services available, we are here to provide it. Workshops: RIYADH - Dammam Road Near MVPI KHOBAR - El Thoqba Industrial \u200b Also we proudly serve the following regions in KSA: Southern and Western Regions For any inquiries, questions, please call: 920008422 or fill out the following form Thanks for submitting! Alloy Wheel Repair Specialists P.O.Box 341427 Riyadh 11333, Saudi Arabia info@alloywheel.sa To apply for a job with AWRS, please send a cover letter together with your C.V. to: info@alloywheel.sa \u00a9 2022 By AWRS SAUDI ARABIA.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da43707f-ac52-44e4-af89-c812a627330b": {"__data__": {"id_": "da43707f-ac52-44e4-af89-c812a627330b", "embedding": null, "metadata": {"tag": "p", "url": "https://www.allrecipes.com/recipe/10813/best-chocolate-chip-cookies/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca363537-6944-40ca-bcba-412941039b40", "node_type": "4", "metadata": {"url": "https://www.allrecipes.com/recipe/10813/best-chocolate-chip-cookies/"}, "hash": "7247577abb1b872af32b896b6e4a4bde75a919ffadaa39a6fc668a82a6924c12", "class_name": "RelatedNodeInfo"}}, "text": "This chocolate chip cookie recipe makes delicious cookies with crisp edges and chewy middles. This chocolate chip cookie recipe is truly the best. Just take it from the 14,000 members of the Allrecipes community who have given it rave reviews! These chocolate chip cookies are beloved because they're soft, chewy, and absolutely irresistible. Our top-rated recipe for chocolate chip cookies will quickly become your go-to. Making bakery-worthy chocolate chip cookies is much easier than it seems. You'll find a detailed ingredient list and step-by-step instructions in the recipe below, but let's go over the basics: These are the kitchen staples you'll need to make the best chocolate chip cookies of your life: Here's a very brief overview of what you can expect when you make chocolate chip cookies from-scratch: In an oven preheated to 350 degrees F, the chocolate chip cookies should be perfectly baked in about 10 minutes. The edges should be golden brown and the cookies should be mostly set (they'll continue to set as the cool). Allrecipes Video \u201cEveryone needs a good chocolate chip cookie recipe,\u201d says culinary producer Nicole McLaughlin (a.k.a. NicoleMcMom ). Here are a few of her expert tips and tricks for making perfect chocolate chip cookies every time: Store the cooled chocolate chip cookies in an airtight container at room temperature for up to a week. If you want to go the extra mile, throw a piece of white bread into the container \u2014 it'll absorb the dry air and keep the cookies soft for longer. Learn more : How to Store Cookies So They Stay Fresh Yes! You can freeze baked chocolate chip cookies and chocolate chip cookie dough. Learn more : How to Freeze Cookies and Cookie Dough for Easy Baking \"This has been my go-to chocolate chip cookie recipe since I found it on Allrecipes several years ago,\" says GCKJA . \"The recipe is not the 'back of the bag' recipe, with different ratios of all ingredients and that's what makes these cookies so darned good!\" \"By far my favorite chocolate chip cookie recipe,\" raves Sonja Ellingson . \"Exactly the way I like them. Chewy, crispy and thin. I added some toffee chips in the second batch and they were over the top good!\" \"My go-to recipe,\" says Andrea Howard . \"I sometimes add oatmeal, nuts, raisins, etc. but they are also fantastic without. Crunchy and chewy in every bite!\" 1 cup butter, softened 1 cup white sugar 1 cup packed brown sugar 2 eggs 2 teaspoons vanilla extract 1 teaspoon baking soda 2 teaspoons hot water \u00bd teaspoon salt 3 cups all-purpose flour 2 cups semisweet chocolate chips 1 cup chopped walnuts Gather your ingredients, making sure your butter is softened, and your eggs are room temperature. Dotdash Meredith Food Studios Preheat the oven to 350 degrees F (175 degrees C). Beat butter, white sugar, and brown sugar with an electric mixer in a large bowl until smooth. Dotdash Meredith Food Studios Beat in eggs, one at a time, then stir in vanilla. Dotdash Meredith Food Studios Dissolve baking soda in hot water. Add to batter along with salt. Dotdash Meredith Food Studios Stir in flour, chocolate chips, and walnuts. Dotdash Meredith Food Studios Drop spoonfuls of dough 2 inches apart onto ungreased baking sheets. Dotdash Meredith Food Studios Bake in the preheated oven until edges are nicely browned, about 10 minutes. Dotdash Meredith Food Studios Cool on the baking sheets briefly before removing to a wire rack to cool completely. Dotdash Meredith Food Studios Store in an airtight container or serve immediately and enjoy! Dotdash Meredith Food Studio * Percent Daily Values are based on a 2,000 calorie diet. Your daily values may be higher or lower depending on your calorie needs. ** Nutrient information is not available for all ingredients. Amount is based on available nutrient data. (-) Information is not currently available for this nutrient. If you are following a medically restrictive diet, please consult your doctor or registered dietitian before preparing this recipe for personal consumption. Powered by the ESHA Research Database \u00a9 2018, ESHA Research, Inc. All Rights Reserved", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebe08b61-8b48-4a53-a48f-f1ef939e6310": {"__data__": {"id_": "ebe08b61-8b48-4a53-a48f-f1ef939e6310", "embedding": null, "metadata": {"tag": "p", "url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dcdff1e-32e9-4e81-87e0-091f47ce0e76", "node_type": "4", "metadata": {"url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}, "hash": "e6f584d17cb92b707b0a26d1f12aa005ac04901dbeca1513599c1f1a0b83a4cf", "class_name": "RelatedNodeInfo"}}, "text": "We can almost guarantee you won\u2019t have leftovers\u2014this is the best chocolate chip cookie recipe around! If you do end up with any extra chocolate chip cookies, they can easily be stored for later enjoyment. To keep your leftover cookies chewy and soft, store them at room temperature in resealable food-storage plastic bags or stacked in tightly covered containers. If you want to freeze your homemade chocolate chip cookies instead, wait until they\u2019ve cooled completely. Stack them in between layers of wax paper in a covered freezer container or plastic freezer bag and freeze your cookies for up to 2 months. To thaw, keep them covered at room temperature for 1 to 2 hours. Go ahead and chow down! Want to switch it up? This chocolate chip cookie recipe can easily be customized with endless flavor combinations! Just replace the chocolate chips and nuts with the same amount of the new ingredients you want to add. Try adding macadamia nuts and white vanilla baking chips, or pack a double punch of peanut flavor with peanut butter chips and chopped peanuts. Make salted butterscotch-pecan cookies by swapping in butterscotch chips and chopped pecans, and then sprinkling your cookies with coarse salt before baking. We also have plenty of other chocolate chip cookie recipes to try. Can\u2019t decide between cookies and brownies? Our Double Chocolate Chip Cookies , are packed full of decadent chocolate flavor. Our Oatmeal Chocolate Chip Cookies , offer next-level deliciousness, too\u2014you really can never go wrong with a classic! Baking is a science, and we can prove it! Whether you prefer a soft, chewy chocolate chip cookie or a crisp, crunchy cookie, consider the factors that make a difference in cookie structure\u2014from the ratio and type of sugars used to the amount of butter or shortening used, and how much flour is stirred in. If you like your cookies slightly crispy on the outside and chewy on the inside\u2014what many people consider the perfect texture for a chocolate chip cookie\u2014just follow this recipe exactly! Our Ultimate Chocolate Chip Cookies are truly the best chocolate chip cookies around\u2014they\u2019re called \u201cultimate\u201d for a reason. Prefer your homemade chocolate chip cookies crispy and thin? Cut out the brown sugar completely, and increase the amount of granulated sugar to 1 1/2 cups. Granulated sugar contains less moisture and helps cookies spread as they bake. After taking your chocolate chip cookies out of the oven, let them cool on a cookie sheet for five minutes. The cookies will keep baking, allowing them to crisp up further as the moisture evaporates. If you want your chocolate chip cookies airy, soft and cakey, we\u2019ve got you covered. Add in an extra egg for additional moisture and structure, and add in 2 tablespoons of milk with the egg to help soften the dough as it bakes and give the cookie a cake-like crumb. Increase the amount of flour to 3 cups to help the cookies rise (rather than spread). The perfectly cakey chocolate chip cookie is just a few steps away! Want to have a crowd-favorite cookie always at the ready? Follow this chocolate chip cookie recipe to get the perfect dough, and freeze it for later baking! Freeze individual unbaked dough balls on cookie sheets. Once the dough balls are frozen, place them in plastic freezer bags and freeze them for up to 2 months. When you\u2019re ready to bake your cookies, take the frozen cookie dough balls from the freezer and place them on a cookie sheet. Let them sit at room temperature for 15 minutes as you preheat the oven. Keep an eye on your chocolate chip cookies as they bake\u2014the bake time might need to be adjusted slightly. This hack guarantees delicious cookies in a snap!", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d068ad3-382f-4cb9-a8ff-1561de776933": {"__data__": {"id_": "8d068ad3-382f-4cb9-a8ff-1561de776933", "embedding": null, "metadata": {"tag": "p", "url": "https://www.cobone.com/en/deals/auto-riyadh/1-rims-colour-zoom-car/126639"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2be8f15d-c0f3-4503-be54-0d33ea4b1e11", "node_type": "4", "metadata": {"url": "https://www.cobone.com/en/deals/auto-riyadh/1-rims-colour-zoom-car/126639"}, "hash": "f7efbec09297a121cf2c359b0e44e046aa6cd127612b648ccb711ff4a01b9677", "class_name": "RelatedNodeInfo"}}, "text": "OR Be the first to hear about", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "720469a2-ba11-4992-816d-6e1a9cf1748d": {"__data__": {"id_": "720469a2-ba11-4992-816d-6e1a9cf1748d", "embedding": null, "metadata": {"tag": "p", "url": "https://www.ibm.com/topics/machine-learning"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43a46798-f605-4dd1-b6d8-0d5da8ac6f2f", "node_type": "4", "metadata": {"url": "https://www.ibm.com/topics/machine-learning"}, "hash": "4e16241a77e260154e0b93e75342b7081fe0b9c441eaa5ea24f17bc6287a43ed", "class_name": "RelatedNodeInfo"}}, "text": "Machine learning (ML) is a branch of artificial intelligence (AI) and computer science that focuses on the using data and algorithms to enable AI to imitate the way that humans learn, gradually improving its accuracy. UC Berkeley (link resides outside ibm.com) breaks out the learning system of a machine learning algorithm into three main parts. Explore the benefits of generative AI and ML and learn how to confidently incorporate these technologies into your business. Register for the white paper on AI governance Since deep learning and machine learning tend to be used interchangeably, it\u2019s worth noting the nuances between the two. Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence. However, neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks. The way in which deep learning and machine learning differ is in how each algorithm learns. \"Deep\" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn\u2019t necessarily require a labeled dataset. The deep learning process can ingest unstructured data in its raw form (e.g., text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of large amounts of data. You can think of deep learning as \"scalable machine learning\" as Lex Fridman notes in this MIT lecture (link resides outside ibm.com). Classical, or \"non-deep,\" machine learning is more dependent on human intervention to learn. Human experts determine the set of features to understand the differences between data inputs, usually requiring more structured data to learn. Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. The \u201cdeep\u201d in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers\u2014which would be inclusive of the input and the output\u2014can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network. Deep learning and neural networks are credited with accelerating progress in areas such as computer vision, natural language processing, and speech recognition. See the blog post \u201c AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What\u2019s the Difference? \u201d for a closer look at how the different concepts relate. Explore the watsonx.ai interactive demo Download \"Machine learning for Dummies\" Explore Gen AI for developers Machine learning models fall into three primary categories. Supervised learning , also known as supervised machine learning, is defined by its use of labeled datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting . Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, na\u00efve bayes, linear regression, logistic regression, random forest, and support vector machine (SVM). Unsupervised learning , also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets (subsets called clusters). These algorithms discover hidden patterns or data groupings without the need for human intervention. This method\u2019s ability to discover similarities and differences in information make it ideal for exploratory data analysis, cross-selling strategies, customer segmentation, and image and pattern recognition. It\u2019s also used to reduce the number of features in a model through the process of dimensionality reduction. Principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering, and probabilistic clustering methods. Semi-supervised learning offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of not having enough labeled data for a supervised learning algorithm. It also helps if it\u2019s too costly to label enough data. For a deep dive into the differences between these approaches, check out \" Supervised vs. Unsupervised Learning: What's the Difference? \" Reinforcement machine learning is a machine learning model that is similar to supervised learning, but the algorithm isn\u2019t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem. The IBM Watson\u00ae system that won the Jeopardy! challenge in 2011 is a good example. The system used reinforcement learning to learn when to attempt an answer (or question, as it were), which square to select on the board, and how much to wager\u2014especially on daily doubles. Learn more about reinforcement learning A number of machine learning algorithms are commonly used. These include: Depending on your budget, need for speed and precision required, each algorithm type \u2014supervised, unsupervised, semi-supervised, or reinforcement\u2014has its own advantages and disadvantages. For example, decision tree algorithms are used for both predicting numerical values (regression problems) and classifying data into categories. Decision trees use a branching sequence of linked decisions that may be represented with a tree diagram. A prime advantage of decision trees is that they are easier to validate and audit than a neural network. The bad news is that they can be more unstable than other decision predictors. Overall, there are many advantages to machine learning that businesses can leverage for new efficiencies. These include machine learning identifying patterns and trends in massive volumes of data that humans might not spot at all. And this analysis requires little human intervention: just feed in the dataset of interest and let the machine learning system assemble and refine its own algorithms\u2014which will continually improve with more data input over time. Customers and users can enjoy a more personalized experience as the model learns more with every experience with that person. On the downside, machine learning requires large training datasets that are accurate and unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering sufficient data and having a system robust enough to run it might also be a drain on resources. Machine learning can also be prone to error, depending on the input. With too small a sample, the system could produce a perfectly logical algorithm that is completely wrong or misleading. To avoid wasting budget or displeasing customers, organizations should act on the answers only when there is high confidence in the output. Here are just a few examples of machine learning you might encounter every day: Speech recognition: It is also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, and it is a capability which uses natural language processing (NLP) to translate human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search\u2014e.g. Siri\u2014or improve accessibility for texting. Customer service: Online chatbots are replacing human agents along the customer journey, changing the way we think about customer engagement across websites and social media platforms. Chatbots answer frequently asked questions (FAQs) about topics such as shipping, or provide personalized advice, cross-selling products or suggesting sizes for users. Examples include virtual agents on e-commerce sites; messaging bots, using Slack and Facebook Messenger; and tasks usually done by virtual assistants and voice assistants. Computer vision: This AI technology enables computers to derive meaningful information from digital images, videos, and other visual inputs, and then take the appropriate action. Powered by convolutional neural networks, computer vision has applications in photo tagging on social media, radiology imaging in healthcare, and self-driving cars in the automotive industry. Recommendation engines: Using past consumption behavior data, AI algorithms can help to discover data trends that can be used to develop more effective cross-selling strategies. Recommendation engines are used by online retailers to make relevant product recommendations to customers during the checkout process. Robotic process automation (RPA ): Also known as software robotics, RPA uses intelligent automation technologies to perform repetitive manual tasks. Automated stock trading: Designed to optimize stock portfolios, AI-driven high-frequency trading platforms make thousands or even millions of trades per day without human intervention. Fraud detection: Banks and other financial institutions can use machine learning to spot suspicious transactions. Supervised learning can train a model using information about known fraudulent transactions. Anomaly detection can identify transactions that look atypical and deserve further investigation. As machine learning technology has developed, it has certainly made our lives easier. However, implementing machine learning in businesses has also raised a number of ethical concerns about AI technologies. Some of these include: While this topic garners a lot of public attention, many researchers are not concerned with the idea of AI surpassing human intelligence in the near future. Technological singularity is also referred to as strong AI or superintelligence. Philosopher Nick Bostrum defines superintelligence as \u201cany intellect that vastly outperforms the best human brains in practically every field, including scientific creativity, general wisdom, and social skills.\u201d Despite the fact that superintelligence is not imminent in society, the idea of it raises some interesting questions as we consider the use of autonomous systems, like self-driving cars. It\u2019s unrealistic to think that a driverless car would never have an accident, but who is responsible and liable under those circumstances? Should we still develop autonomous vehicles, or do we limit this technology to semi-autonomous vehicles which help people drive safely? The jury is still out on this, but these are the types of ethical debates that are occurring as new, innovative AI technology develops. While a lot of public perception of artificial intelligence centers around job losses, this concern should probably be reframed. With every disruptive, new technology, we see that the market demand for specific job roles shifts. For example, when we look at the automotive industry, many manufacturers, like GM, are shifting to focus on electric vehicle production to align with green initiatives. The energy industry isn\u2019t going away, but the source of energy is shifting from a fuel economy to an electric one. In a similar way, artificial intelligence will shift the demand for jobs to other areas. There will need to be individuals to help manage AI systems. There will still need to be people to address more complex problems within the industries that are most likely to be affected by job demand shifts, such as customer service. The biggest challenge with artificial intelligence and its effect on the job market will be helping people to transition to new roles that are in demand. Privacy tends to be discussed in the context of data privacy, data protection, and data security. These concerns have allowed policymakers to make more strides in recent years. For example, in 2016, GDPR legislation was created to protect the personal data of people in the European Union and European Economic Area, giving individuals more control of their data. In the United States, individual states are developing policies, such as the California Consumer Privacy Act (CCPA), which was introduced in 2018 and requires businesses to inform consumers about the collection of their data. Legislation such as this has forced companies to rethink how they store and use personally identifiable information (PII). As a result, investments in security have become an increasing priority for businesses as they seek to eliminate any vulnerabilities and opportunities for surveillance, hacking, and cyberattacks. Instances of bias and discrimination across a number of machine learning systems have raised many ethical questions regarding the use of artificial intelligence. How can we safeguard against bias and discrimination when the training data itself may be generated by biased human processes? While companies typically have good intentions for their automation efforts, Reuters (link resides outside ibm.com) highlights some of the unforeseen consequences of incorporating AI into hiring practices. In their effort to automate and simplify a process, Amazon unintentionally discriminated against job candidates by gender for technical roles, and the company ultimately had to scrap the project. Harvard Business Review (link resides outside ibm.com) has raised other pointed questions about the use of AI in hiring practices, such as what data you should be able to use when evaluating a candidate for a role. Bias and discrimination aren\u2019t limited to the human resources function either; they can be found in a number of applications from facial recognition software to social media algorithms. As businesses become more aware of the risks with AI, they\u2019ve also become more active in this discussion around AI ethics and values. For example, IBM has sunset its general purpose facial recognition and analysis products. IBM CEO Arvind Krishna wrote: \u201cIBM firmly opposes and will not condone uses of any technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and Principles of Trust and Transparency.\u201d Since there isn\u2019t significant legislation to regulate AI practices, there is no real enforcement mechanism to ensure that ethical AI is practiced. The current incentives for companies to be ethical are the negative repercussions of an unethical AI system on the bottom line. To fill the gap, ethical frameworks have emerged as part of a collaboration between ethicists and researchers to govern the construction and distribution of AI models within society. However, at the moment, these only serve to guide. Some research (link resides outside ibm.com) shows that the combination of distributed responsibility and a lack of foresight into potential consequences aren\u2019t conducive to preventing harm to society. Read more about IBM's position on AI Ethics Selecting a platform can be a challenging process, as the wrong system can drive up costs, or limit the use of other valuable tools or technologies. When reviewing multiple vendors to select an AI platform, there is often a tendency to think that more features = a better system. Maybe so, but reviewers should start by thinking through what the AI platform will be doing for their organization. What machine learning capabilities need to be delivered and what features are important to accomplish them? One missing feature might doom the usefulness of an entire system. Here are some features to consider. Reimagine how you work with AI: our diverse, global team of more than 20,000 AI experts can help you quickly and confidently design and scale AI and automation across your business, working across our own IBM watsonx technology and an open ecosystem of partners to deliver any AI model, on any cloud, guided by ethics and trust. Operationalize AI across your business to deliver benefits quickly and ethically. Our rich portfolio of business-grade AI products and analytics solutions are designed to reduce the hurdles of AI adoption and establish the right data foundation while optimizing for outcomes and responsible use. Multiply the power of AI with our next-generation AI and data platform. IBM watsonx is a portfolio of business-ready tools, applications and solutions, designed to reduce the costs and hurdles of AI adoption while optimizing outcomes and responsible use of AI. Granite is IBM\u2019s flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. Learn the fundamental concepts for AI and generative AI, including prompt engineering, large language models and the best open source projects. AI technology has been rapidly evolving over the last couple of decades. Learn how businesses are implementing AI today. Learn tools businesses use to efficiently run and manage AI models and empower their data scientist with technology that can help optimize their data-driven decision making. Explore how machine learning projects help you continually learn from data and predict the future. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bf1b395-52e0-4cac-9594-95fea9af628f": {"__data__": {"id_": "6bf1b395-52e0-4cac-9594-95fea9af628f", "embedding": null, "metadata": {"tag": "p", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e3f523f-46e7-4fcc-bfd0-d7eca1da0fbc", "node_type": "4", "metadata": {"url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "765288ceda300671d9a3cb20c5e9dc6611111260ba903577189a342232469518", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 LlamaIndex LlamaIndex \u2022 Nov 14, 2023 Introducing create-llama , the easiest way to get started with LlamaIndex! Update 2023\u201311\u201320: we now have a guide to deploying your create-llama apps ! Want to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We\u2019ve created a simple to use command-line tool that will generate a full-stack app just for you \u2014 just bring your own data! To get started, run: The app will then ask you a series of questions about what kind of app you want. You\u2019ll need to supply your own OpenAI API key (or you can customize it to use a different LLM), and make a few decisions. The generated app has a data folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you\u2019re using LlamaIndex.TS as the back-end (see below), you\u2019ll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you\u2019re using the Python backend, you can read even more types, including audio and video files! The front-end it generates is a Next.js application, with your choice of shadcn/ui or vanilla HTML and CSS for styling. For the back-end, you have 3 options: There are a couple of other questions you\u2019ll be asked: Once you\u2019ve got your app up and running, you can customize it to your heart\u2019s content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you\u2019d like to use GPT-4 you can configure that by modifying the file app/api/chat/llamaindex-stream.ts (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local. LlamaIndex Newsletter 2024-04-02 2024-04-02 LlamaIndex Newsletter 2024-03-26 2024-03-26 Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations 2024-03-19 LlamaIndex Newsletter 2024-03-19 2024-03-19 \u00a9 2024 LlamaIndex Privacy Notice Terms of Service", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fee7fa61-ff8a-4a7b-9ad6-bedd01d90ff8": {"__data__": {"id_": "fee7fa61-ff8a-4a7b-9ad6-bedd01d90ff8", "embedding": null, "metadata": {"tag": "p", "url": "https://www.nytimes.com/wirecutter/reviews/best-mascara/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e20b7ead-1753-4937-b759-891824c6f8a6", "node_type": "4", "metadata": {"url": "https://www.nytimes.com/wirecutter/reviews/best-mascara/"}, "hash": "2de6671a88fc674ce214d392f87ed118ce5b291ce43b21ba8fa2eaf37e3b12f1", "class_name": "RelatedNodeInfo"}}, "text": "Advertisement We independently review everything we recommend. When you buy through our links, we may earn a commission. Learn more\u203a By Meirav Devash We've rounded up all the best early, actually worthwhile deals on Wirecutter picks here . Plus: Sign up for Wirecutter's newsletter to get exclusive deals sent straight to your inbox. After a fresh round of testing, we have all-new picks. Bad mascara can ruin your day, clumping your eyelashes into a spidery mess, smudging into raccoon-like circles by 3 p.m., or flaking down your face. And spending more doesn\u2019t insulate you from buying a dud. With dozens of brands and hundreds of different formulas available, you can find both terrible and awesome mascaras from $6 to $60. That\u2019s where we come in. To find truly great mascara, we tested more than 30 options, sent 12 finalists to a diverse panel for brand-concealed testing, and ended up with five remarkable standouts, including an excellent everyday mascara , a sleek tubing option , an exceptional lengthening mascara you can get at the drugstore , a vampy dramatic formula , and a reliable waterproof choice . We hope there\u2019s one on our list that you love, too. Advertisement A buildable formula and a fluffy bristle brush team up to accentuate lashes\u2019 depth and length, without leaving them heavy or gunked up. Rare Beauty Perfect Strokes Universal Volumizing Mascara creates lush volume and noticeable length, and it stays put all day without running or crumbling. Rare Beauty was founded by Selena Gomez, and we were curious if the brand\u2019s renown had more to do with celebrity than quality, but this mascara exceeded our expectations, doing everything well and nothing poorly. It volumizes and lengthens. Many of the mascaras we tried did only one or the other. Testers raved about the volume they achieved with this Rare Beauty mascara; true to its name, the thick formula had a firm texture, and the bushy brush deposited a lot with each stroke, key to creating volume. But the mascara also deftly coated the entirety of testers\u2019 lashes, making them look longer, too. It wasn\u2019t the most lengthening mascara we tried\u2014that award goes to our other picks from Thrive Causemetics and Maybelline \u2014but it did a darn good job in that regard. \u201cThe length was solid,\u201d a panelist summarized. Similar mascaras we tried (thick formula, fiber brush), such as Gucci Mascara L\u2019Obscur , added volume at the lashline but didn\u2019t deposit enough of the mascara on the tips. Others made the tips stick together in an unpleasant, leggy way. \u201cAs someone with sparser, shorter lashes, I really liked this one. The volume is great, and the length is pretty decent, too,\u201d said associate staff writer Annie Chou. You get a lot from one coat. When it first hits the lashes, the thick Rare Beauty mascara is clumpy. But after even just the first comb-through, the pigment distributes evenly while maintaining its heft, creating a sturdy foundation at the base of the lashes with very little actual mascara. You get a lot of value in this long-lasting tube. It feels weightless, and it stays on. Sometimes, the very bulk that helps volumize lashes weighs them down, and that heaviness makes eyes appear smaller, not bigger. All that mascara is also prone to flaking off. In contrast, this lightweight formula doesn\u2019t feel heavy, it leaves lashes lifted, and it\u2019s really on there. \u201cIt stayed on quite well in 90-degree heat,\u201d noted senior updates writer Sri Rain Stewart. \u201cRemoval was pretty easy, too.\u201d Colors: black Variations: mini Price: 0.27-ounce mini tube for $11; 0.45-ounce tube for $20 This tubing mascara encases lashes with a shiny black film that elongates and doesn\u2019t smudge. It\u2019s not officially waterproof, but it\u2019s exceptionally durable and long-lasting. Thrive Causemetics Liquid Lash Extensions Mascara is an inky black mascara that leaves lashes decisively separated and noticeably lengthened. Though it rinses away with water and a little scrub, it\u2019s singularly long-wearing. Thrive heavily markets the mascara as a \u201ctubing\u201d formula, and prior to testing we weren\u2019t certain that would mean much of anything. But it did. It adds length to each lash. Whereas most mascaras we wore coated existing lashes, essentially making them stand out more, the Thrive Causemetics mascara extended length beyond the natural hair shaft. Traditional mascaras color lashes with pigment dissolved in a water and wax or oil base, painting each hair shaft. Tubing formulas work a little differently, carrying pigment in a film-forming polymer base that wraps each hair in microscopic \u201ctubes\u201d of color to physically lengthen the lashes. \u201cThe length was beautiful,\u201d one tester noted. Another said her lashes looked \u201cmagically\u201d longer: \u201cThis is the best lengthener, hands down,\u201d she wrote. The brush is nimble and gives great definition. The plastic brush is shaped like a tidy, narrow Christmas tree that has had its top lobbed off, and the handle has a heavy and purposeful weight. You can feel a little resistance when tugging the wand from the tube, a sign that the wiping disc is ensuring that it doesn\u2019t pick up too much mascara. All of those factors work together to deposit a precise amount of mascara onto lashes without smudging, clumping, or unevenness. Lashes turn out sleek and evenly coated in a way that\u2019s both believable and noticeable. The pigment is true black. Unlike with any of the other mascaras our panelists tested, this mascara\u2019s black shade, called Brynn, contains a single coloring agent: iron oxides CI 77499. (Our other picks brighten or lighten true black with pigments such as ultramarine or zinc oxide.) Senior editor Jennifer Hunter put it simply: \u201cWow, this is really black.\u201d It\u2019s water-resistant and long-wearing. After one tester experienced a morning sob, took a restorative steamy shower, and followed up with an even steamier romantic encounter, they saw zero black streaks, just a little less fullness. If you\u2019re hunting for a mascara that\u2019s waterproof-ish\u2014that is, it lasts all day but doesn\u2019t require an oil-based cleanser for removal\u2014this Thrive Causemetics mascara is just the ticket. It does require a somewhat precise removal technique, though. After wetting your lashes for 30 seconds, you must apply a gentle downward swoop or pinch to pull off the tubes. It\u2019s not difficult, but it takes a bit of practice. Colors: black, brown, blue, plus limited-edition purple, green, pink Price: 0.38-ounce tube for about $25 Advertisement With a nimble, bendy brush and a satin-slick formula, this drugstore mascara delivers long, perfectly delineated lashes. You can find about a billion drugstore mascaras out there, but Maybelline Lash Sensational Sky High is our favorite. Maybelline\u2019s marketing claims that it\u2019s both a lengthening and volumizing formula, but we found it to be exceptional in the former regard. It gave our testers long, deftly separated lashes in a shiny, believable black without transferring or crumbling. At a typical price of about $13 a tube, it\u2019s also the most affordable and widely available of our picks. Lashes look longer, but not in a fake way. The formula is slick and wet, so it evenly covers even the most feathery, wispy hairs from root to tip. Our testers nearly unanimously reported that one coat made their lashes seem longer, plain and simple\u2014not as if they were wearing a full row of false lashes, but as if their own lashes were elongated. The brush is precise. The flexible brush in this Maybelline mascara is bendy and densely dotted with spikes\u2014longer at the base, shorter at the tip\u2014which together make it amazing at delivering generous, even coverage. It enhances and defines every millimeter of natural lashes and creates great separation. It\u2019s almost identical to the Thrive Causemetics mascara brush, at half the price. The color looks natural. We found the standard shade, called blackest black, to be an inky onyx that didn\u2019t look artificial. \u201cIt was obvious I was wearing makeup, but I liked that it still gave more of a natural look,\u201d said one tester, who has no pigmentation in one set of their lashes. It also stayed shiny, not matte, once dry. Colors: three shades of black (blackest, cosmic, very), brown, burgundy, pink, blue Variations: waterproof , tinted primer Price: 0.24-ounce tube for $9 to $13 The combination of a rich formula and a curvy fibrous brush leaves lashes full at the base and defined at the tips. Adding coats intensifies the result without making it heavy or goopy. Too Faced Better Than Sex Volumizing Mascara is usually the most expensive mascara of our picks by a few dollars, and it\u2019s worth the splurge since it does so much, so well. One coat provides a natural boost; a few more can achieve a thick, lush fringe. This showy mascara is ideal for a day out or a special event, or as an everyday mascara for someone accustomed to flashier looks. The brush is big and bold. Of the 12 mascaras we sent to panelists, this Too Faced mascara had the biggest, fluffiest, densest brush. Too Faced\u2019s co-founder confirmed that the brush\u2019s hourglass shape was inspired by Marilyn Monroe\u2019s measurements. Lore aside, the grippy brush makes it easy to pile on a lot of mascara with no smudges, but it\u2019s equally excellent for comb-through separation. To be sure, it doesn\u2019t precisely etch every single lash\u2014that\u2019s more Thrive Causemetics\u2019s vibe\u2014but that isn\u2019t the point here. Each swipe deposits a lot of mascara, but it\u2019s not heavy or messy. When you pull the brush from the tube, it has a startling lack of resistance, which means that the wiping disc allows a lot of the mascara to stay on the fibrous bristles. The creamy Too Faced formula bulks up the base of the lashline, even more so than our pick from Rare Beauty . The effect isn\u2019t subtle or natural\u2014people will know that you\u2019re wearing mascara, and a very pretty one at that. It\u2019s the most buildable. This mascara has a pliant, flexible formula that covers every lash. One coat is enough for some people. But if you keep going with two or three, you get even more fullness and length, with no clumps, tangles, or the unfortunate spider-effect of over-mascaraed lashes. Even more impressive, no one on our testing panel experienced the transfer that\u2019s common with other thickening mascaras we tested, such as Essence Lash Princess False Lash Effect Mascara , which left smudges under our eyebrows. It goes all night. Unlike some of the other dramatic volumizing formulas we tried, which looked great in the mirror but crumbled and smudged in the first hour of real-life wear, this Too Faced mascara stayed put. For a non-waterproof mascara, it held up nicely to a sweaty time between the sheets and a 30-minute Peloton session. (Not in the same day or on the same tester.) Colors: black, brown Variations: travel size , waterproof , \u201c naturally-derived \u201d Price: 0.16-ounce travel-size tube for $16; 0.27-ounce tube for about $30 This truly waterproof formula gives lashes a subtle boost and is ideal for active wearers. The curved brush is a pleasure to use. But the formula\u2019s nylon fibers, which add bulk and length, could irritate sensitive eyes. Most people don\u2019t need to use waterproof mascara every day, especially since such formulas are harder to remove. But Eyeko\u2019s Sport Waterproof Mascara was too well-reviewed to ignore, and we\u2019re glad we didn\u2019t, since it\u2019s an excellent mascara that just happens to be waterproof. Eyeko\u2019s Sport mascara is different in a few significant ways: In addition to being officially waterproof, it comes in a squeezy tube and has a curved brush, and the formula contains nylon fibers for fullness and elongation. All of those factors team up to boost fullness and length and leave lashes looking natural and unfussy. Plus, you can swim, shower, and bawl in it with nary a smudge. The curved brush gives lashes exceptional coverage. The Eyeko mascara brush is shaped like the curve of an eye. That intuitive line makes it easy to paint the outermost corners of the lashes, something no other mascara did quite as well. One panelist couldn\u2019t stop raving about how well she was able to reach her teeny bottom lashes, maybe more effectively than with any other mascara ever: \u201cI\u2019d use it just for that!\u201d The nylon adds volume and length. Look closely at the swirl of mascara on the tip of the Eyeko brush, and you\u2019ll see tiny hair-like fibers. Those bits of nylon work to physically bulk up the thickness of the lashes and add length. We found that the fibers better served to add length, with one tester noting that it made her lashes swoop up to graze her eyelids\u2014in a good way. It has a satisfying build. The first coat of this Eyeko mascara is subtle, as it bestows a slight darkening, a bit of density, and a touch of length to lashes. But because the formula doesn\u2019t dry too quickly, you can gradually dial up the intensity by adding more coats without lashes feeling crispy, heavy, or sticky in the process. That\u2019s unusual in comparison with most waterproof or long-wearing mascaras we\u2019ve worn, which tend to dry fast and rigid, leaving little room for adjustment. It really is waterproof. We wore this Eyeko mascara in a pool and shower, and not a bit smudged, dripped, or crumbled. Waterproof formulas are more difficult to remove, and our testers who tried to wash with soap and water initially had trouble until they added a makeup remover to their nighttime routine. Those of us who use an oil-based remover as a matter of course had no trouble. Colors: black Price: 0.27-ounce tube for about $20 to $30 I\u2019ve been a beauty journalist since 2004, which means I can barely remember a time when I wasn\u2019t testing a mascara\u2014for editorial consideration, for annual beauty awards, or because it had a new ingredient, an improved formula, a cool wand, a vegan version, or an unexpected color. For this guide, I consulted with other mascara experts, including people who formulate and develop mascaras behind the scenes, plus makeup artists, who use the stuff on more lashes than anyone. When it comes to mascara, I am quite frankly overeducated and tough to impress. Sephora alone sells more than 200 mascaras. So does Target. To whittle down the seemingly limitless selection to worthy contenders, we cross-referenced dozens of best-of mascara lists from beauty and fashion magazines and blogs with top-seller lists on Amazon and beauty sites like Sephora and Ulta. We also scoured makeup forums and Reddit threads, and we poked around mascara-Tok. We spoke to two cosmetic chemists, one product-development expert, and one makeup artist, none of whom had partnerships or affiliations with mascara companies at the time of our interviews. We also polled our network of opinionated and experienced beauty-industry colleagues. The result was a list of more than two dozen mascaras for in-person testing. Experts stressed that marketing language is particularly dubious in this category. Mascaras can\u2019t curl, lift, stretch, or sculpt lashes, for example, but lots of formulas claim that they do. Mascara can, however, make lashes look longer, thicker, and darker. So we ignored packaging claims and instead evaluated every mascara on the following criteria: Over almost two months, we tested the two dozen formulas in daily life, including during workouts, on rainy days, in showers, and through a couple of warm-climate plane trips and pool dips. We rated the mascaras on a scale of one to five in the six categories outlined above, and we provided detailed feedback. Once we tallied the results, 12 mascaras moved on to a panel of testers. To minimize brand loyalty and bias, we obscured the labels and sent them off to four testers. Our panel included a person with unpigmented lashes, one with monolids, and an individual blessed with lashes so thick, they barely need mascara at all. A few testers identified as having sensitive eyes. All of them provided even more feedback on the same rubric as we used in the earlier stage. Some cosmetics and personal-care products, including mascaras, contain a group of chemicals called PFAS\u2014per- and polyfluoroalkyl substances\u2014or forever chemicals , which aid with texture, finish, and longevity. Repeated, long-term exposure to PFAS has been linked to health concerns such as cancer and weakened immune function. People\u2019s use of these chemicals also has environmental implications: Whatever a person puts on their face ultimately washes into the water system and eventually into the nation\u2019s tap water . In the time we\u2019ve been researching this guide, we\u2019ve noted mounting international pressure to ban PFAS in cosmetics, as well as some national movement to ban them altogether. Right now, though, PFAS aren\u2019t regulated in personal-care products. The danger of PFAS exposure through absorption in hair follicles or inadvertent contact with the eyelid is insufficiently studied. One study found higher PFAS levels in people who wore foundation than those who didn\u2019t, suggesting that topical use may be a pathway to exposure. We couldn\u2019t find any research concerning PFAS absorption through mascara. Nonetheless, we cross-checked the ingredient lists of our five picks with the FDA\u2019s list of 35 PFAS that companies reported using in cosmetics . Because of its waterproof formula, Eyeko Sport Waterproof Mascara contains trimethylsiloxysilicate, a polymer that aids in water resistance. This isn\u2019t unusual\u2014many waterproof mascaras rely on PFAS. Our four other selections do not include any of the FDA-identified PFAS. Because no single cosmetic-exposure source is likely to pose a health risk, and because we think some people want a truly waterproof mascara, we decided to keep the Eyeko mascara on our list for the time being. If you\u2019re concerned about PFAS, you have lots of ways to reduce your exposure , including choosing one of our four other picks. Drugstore staple CoverGirl Lash Blast Volume Mascara delivered on its no-clump promise and met our requirements for length, fullness, and lift. However, the thick, spiky, molded brush tended to smudge. Building volume was a challenge for our testers, and fine lashes could droop by day\u2019s end. On the plus side, it\u2019s one of the easiest mascaras to remove. At just five bucks, Amazon best seller and TikTok sensation Essence Lash Princess False Lash Effect Mascara is a great value and the least expensive mascara we tried. It definitely outperformed its cost in our tests, but reports of clumps, light smudging, smearing, and flaking made us hesitate to recommend it. We gave L\u2019Or\u00e9al Telescopic Original Mascara solid marks for boosting length and volume, but our testers found the wand polarizing. It has a built-in comb for detangling. That\u2019s a plus. But most testers strongly disliked the thin, flexible wand and molded sawtooth brush, which they found difficult to use. A small group liked the brush; one tester said she was \u201cobsessed.\u201d Tarte Big Ego Vegan Mascara , which has girl-next-door vibes, went on a little sticky but came off easily with a wipe. Benefit Cosmetics BADgal Bang brought more va-va-voom to lashes but tended to collect under eyes by day\u2019s end. Ami Col\u00e9 Lash Amplifying Mascara lengthened more than it volumized (with a side of clumps), though it held up to tears as long as our testers didn\u2019t rub their eyes. A note for tubing-mascara fans: Tarte Tartelette Tubing Mascara built enviable body, but the tubes didn\u2019t glide off as cleanly as other tubing formulas in our tests. Other suitable everyday mascaras with not-so-fatal flaws include Glossier Lash Slick , which darkened and defined lashes but didn\u2019t offer any wow factor even when we piled it on. After three coats, Charlotte Tilbury Pillow Talk Push Up Lashes Mascara created feathery\u2014not spidery\u2014volume but lost its luster by the end of the day. And It Cosmetics Superhero Elastic Stretch Volumizing and Lengthening Mascara had issues from start to finish, beginning with clumps and ending with flakes. For lush, evening looks, we had high hopes for Gucci Mascara L\u2019Obscur , with its luxurious peach-and-gold textured tube. It just looked and felt expensive. But the result didn\u2019t meet our expectations, and it took a bit of work to remove. Plus, one tester found the formula irritating. Dior\u2019s Diorshow thickened lashes with tarantula-esque flair but raccooned under testers\u2019 eyes under humid conditions. We were divided on Pat McGrath FetishEyes Mascara . It pilled at first, but once that was sorted out, it stayed on forever and produced impressive color and fullness. Despite its billing as a lengthening mascara, the length was nothing to write home about, especially for the price. The unusual 2.5 mm slim metal wand in NeoGen Dermalogy Maxicara both drew us in and turned us off, as the mascara was annoying to apply and tough to remove. We also deemed the two-step application process for Honest Beauty Extreme Length Mascara + Lash Primer too much of a hassle for its mediocre results. If you are a tubing devotee who leans toward a more subtle lash look than our pick from Thrive Causemetics delivers, consider Kevyn Aucoin\u2019s The Volume Mascara , which in our tests was dark and glossy and defined like crazy. The volume it provided, however, was nothing to get excited over. Blinc Lash Extension Tubing Mascara produced a similarly subtle look upon application, but little flakes collected under the eyes by mid-afternoon. This article was edited by Hannah Morrill and Jennifer Hunter. Katie M. Palmer wrote an earlier version of this guide, first published in 2015. Perry Romanowski, cosmetic chemist and product-development expert , email interview , May 15, 2023 Ni\u2019Kita Wilson, cosmetic chemist and product-development expert , phone interview , June 16, 2023 Alexis Androulakis, TikTok-viral beauty educator and product-development expert , phone interview , June 22, 2023 Francelle Daly, makeup artist and founder of Love+Craft+Beauty , phone interview , June 13, 2023 Meirav Devash Meirav Devash has covered beauty trends for over two decades as a magazine editor and writer, personally testing an unreasonable number of cosmetics, skin-care items, and hair products. She has an encyclopedic knowledge of lipstick and heavy metal. Advertisement Wirecutter is the product recommendation service from The New York Times. Our journalists combine independent research with (occasionally) over-the-top testing so you can make quick and confident buying decisions. Whether it\u2019s finding great products or discovering helpful advice, we\u2019ll help you get it right (the first time).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "542345d9-9fed-4e41-9613-a177356a2630": {"__data__": {"id_": "542345d9-9fed-4e41-9613-a177356a2630", "embedding": null, "metadata": {"tag": "p", "url": "https://www.rac.co.uk/drive/advice/car-maintenance/how-much-does-alloy-wheel-refurbishment-cost/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b23408d-eb4c-49f9-92e1-dc29037423a3", "node_type": "4", "metadata": {"url": "https://www.rac.co.uk/drive/advice/car-maintenance/how-much-does-alloy-wheel-refurbishment-cost/"}, "hash": "893039915c83c75a1c8e1b39824cc442fdad3fd65f6f1c9f37390a719eda503a", "class_name": "RelatedNodeInfo"}}, "text": "Verifying you are human. This may take a few seconds. Waiting for www.rac.co.uk to respond... Please enable Cookies and reload the page.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"4c692992-7ffa-4c8d-8d7e-7fc9ab0269d6": {"doc_hash": "b5edc5d2d2e02abb3b96a9e1d1536ebfdb0a96f1018f6bc096a7139da695f255", "ref_doc_id": "34948ca5-f1a0-4b26-bc2b-89e48e8baef4"}, "d7dc88c2-0a49-4d20-bdc6-ab70a6e9a732": {"doc_hash": "74c41f30c7595434a65adbd5212f176c2ce3a3e0cf62fc05df4e2e92ac743f36", "ref_doc_id": "3a273f6c-7ee2-47b2-9a33-efe902abd2a2"}, "aed79758-aaa9-438f-b3cf-c8e396aacbc5": {"doc_hash": "e4fb4b4e64c90b57d41b3cd4585532b77c4c56c67319ab17ab883d1579d33791", "ref_doc_id": "c8058805-f4bb-4092-8b01-a7d8d1e142a2"}, "1db2cdcc-f091-4b91-8ef6-65ae162b47a8": {"doc_hash": "7e94c4f2cb031d2fec1cb8115de4283f5a0078463a3c6005ebe2209eb84d2d7e", "ref_doc_id": "e9d4219d-8651-4813-a71c-c37b43df0e55"}, "2799b79f-14dc-4799-b496-f7d7ca30209a": {"doc_hash": "7b54cea52cebf8b4fe6f136364518fff51b7d07f7252c3935c506e789dd262da", "ref_doc_id": "ba773043-68e9-41bb-bf37-8468e78e61fa"}, "ab260ffc-9379-4126-8deb-05b4d8d0a6f1": {"doc_hash": "da400fd5dbb0c2350a5904e51b811ee2b7eb8df917d926f9ff056fd902594273", "ref_doc_id": "4edef67d-2227-43fc-b8b4-618a2b53af52"}, "c6cd537c-1ced-4d67-bec6-5d90bbebb76c": {"doc_hash": "62f0593769d8af7f30897ab04188ae24dc2cefc08b11a73cbfea56782eef1ef2", "ref_doc_id": "10084969-a2c5-4bb5-b7d1-d8a08b05d58b"}, "87fba420-19be-4d5b-ab4b-4622d42b6557": {"doc_hash": "e855c6ab1f4a58f27d0a16123801411a66f781d923bf1f962eb2b9dcc4f01711", "ref_doc_id": "8a434e7a-9abb-4432-bc6a-8621b18e9c86"}, "c4bf41c0-b3f7-4fbb-9105-4bd7e1126f62": {"doc_hash": "aac9da820c9c85ee55f914763289e49d90bc13a2d56ecaa469e15c47c1a70026", "ref_doc_id": "c89ded7b-5760-4b9d-ad7f-a557beed000a"}, "ad8a32fb-8eac-4e0d-838b-47210d45831d": {"doc_hash": "3260ed5c1ab08ae6f6323b1c31ab716c1d0c08c7da968379f402e9cf8f86a5b7", "ref_doc_id": "c3d02fa5-7dec-4b91-9e79-d261e0322194"}, "95b002f8-bf16-40b2-8b77-ee96af602c73": {"doc_hash": "86cd8389c628b8ef6a7f2e8969ea82a4209f1727bfbc4503a01052b461e4ae77", "ref_doc_id": "261dd891-15e7-47f7-9fe3-3d73179e2b35"}, "218b28bb-9ae4-4cf9-a750-f88b188f4308": {"doc_hash": "557038f03e02850a929733e846cfa938e70afa7f48c80a8ddd99faf5d21663e4", "ref_doc_id": "5ebfe72f-ec7b-4c88-bd2c-da17f5acf7f2"}, "f7b1b69a-ca6d-4ddd-bd61-3fc532ac311b": {"doc_hash": "63e4dc1c0b0e31daa874376c6774f1d9f0700a6ddc18715b269a612080c66b2d", "ref_doc_id": "063555a2-c6a8-4ba6-8d8b-d52137e29ace"}, "552bf27e-bce5-40bb-9d34-a3bede7a748b": {"doc_hash": "2e9a879f718350fdd26a0b976ee78ff603e239055494f10f18c816fa642822a5", "ref_doc_id": "7b3511f5-b591-4d43-a74f-c21386aa80db"}, "48a1e406-c8df-4700-bd1a-bd73a504c807": {"doc_hash": "9c0601c5a0cb3c862f260def313fe9818e2205e14e1b7a5df388d44b625cca2a", "ref_doc_id": "a47ab726-a49c-4ea5-925c-e1df812cd914"}, "482c8e4a-90fa-4d96-bf1f-2f212921eea5": {"doc_hash": "92f6bc97992584440afadd79e8f5de092012550366cbbbac83c905435544eeaf", "ref_doc_id": "f1b72519-0c6b-4d24-969e-83e11bb57508"}, "82ccf39b-6225-4a9e-8f1e-0040f45be498": {"doc_hash": "a16b33e2198f33ddff3f951886ac909fd28eb4edff80dbf7f2bc890d331cf8be", "ref_doc_id": "caa9fdad-b18c-478e-b162-b89e271ee36e"}, "2368581c-b2cc-4c46-8ef4-cf051fd7fcc5": {"doc_hash": "d9c9d91a7dd1da39ce7dad39842fee110deb488f57e2c92cbfd800b59489d610", "ref_doc_id": "56cd6d27-ff55-45dd-96fb-73f90ff73476"}, "a7ddc733-320f-40c7-a97a-0241900cd290": {"doc_hash": "578776ab9513ac8e764553b1ed7ffe715bc9305312411cb58555da04bfec3625", "ref_doc_id": "7cf9fe29-5ba3-4efb-9fea-86a6edb4b770"}, "31aaf241-ba89-460e-b8bf-6126f34d777b": {"doc_hash": "6c21093c23f8c5ffca7eeac6af2c255bcaa109b9464dc42bf62504561bb26590", "ref_doc_id": "6715c28f-5680-4da9-8800-f98f728af043"}, "1a4bd6df-9c8e-4c9a-bc77-6ecaa7dc40a2": {"doc_hash": "f9d93a33828c63175dced3f3d31e3a885df3e0ebc49e7ac66a4e3f04c40c3eea", "ref_doc_id": "bbb0e710-d7ac-48f0-b3ff-48a16b7e0cdf"}, "da43707f-ac52-44e4-af89-c812a627330b": {"doc_hash": "d5dfea505379c497c349581ecb782839b7edaa4e87bbf47bdc70fc023b7081a5", "ref_doc_id": "ca363537-6944-40ca-bcba-412941039b40"}, "ebe08b61-8b48-4a53-a48f-f1ef939e6310": {"doc_hash": "642f352df5efb490689db512d7bc7e19b44edddc020cf15e2fcadfac7818c37a", "ref_doc_id": "8dcdff1e-32e9-4e81-87e0-091f47ce0e76"}, "8d068ad3-382f-4cb9-a8ff-1561de776933": {"doc_hash": "535402fc583de78474ee68a69c58c261324aa73c5b04a1d22a273ea850122715", "ref_doc_id": "2be8f15d-c0f3-4503-be54-0d33ea4b1e11"}, "720469a2-ba11-4992-816d-6e1a9cf1748d": {"doc_hash": "f24d4dc60b04614218bb34c4a4e4361cb6d9712d1ea43ce43e35e40b354a4d83", "ref_doc_id": "43a46798-f605-4dd1-b6d8-0d5da8ac6f2f"}, "6bf1b395-52e0-4cac-9594-95fea9af628f": {"doc_hash": "2489be251e7e3ae8514f9bce9598565f54c962e9468ae1e32a6300b247db690c", "ref_doc_id": "8e3f523f-46e7-4fcc-bfd0-d7eca1da0fbc"}, "fee7fa61-ff8a-4a7b-9ad6-bedd01d90ff8": {"doc_hash": "e2b5d606db99eae39514794cf22b83db31df1c9a80d611247937a728336ccacb", "ref_doc_id": "e20b7ead-1753-4937-b759-891824c6f8a6"}, "542345d9-9fed-4e41-9613-a177356a2630": {"doc_hash": "e11693504d423df2f268ca0c02b3ddbdb66e4b66faa5e1b9c8e55398b7be7f50", "ref_doc_id": "6b23408d-eb4c-49f9-92e1-dc29037423a3"}}, "docstore/ref_doc_info": {"34948ca5-f1a0-4b26-bc2b-89e48e8baef4": {"node_ids": ["4c692992-7ffa-4c8d-8d7e-7fc9ab0269d6"], "metadata": {"tag": "p", "url": "https://app.slack.com/client/T06PYF8R5J7/C079BRFKJJW"}}, "3a273f6c-7ee2-47b2-9a33-efe902abd2a2": {"node_ids": ["d7dc88c2-0a49-4d20-bdc6-ab70a6e9a732"], "metadata": {"tag": "p", "url": "https://aws.amazon.com/what-is/reinforcement-learning/"}}, "c8058805-f4bb-4092-8b01-a7d8d1e142a2": {"node_ids": ["aed79758-aaa9-438f-b3cf-c8e396aacbc5"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/api_reference/indices/"}}, "e9d4219d-8651-4813-a71c-c37b43df0e55": {"node_ids": ["1db2cdcc-f091-4b91-8ef6-65ae162b47a8"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/indexing/document_management/"}}, "ba773043-68e9-41bb-bf37-8468e78e61fa": {"node_ids": ["2799b79f-14dc-4799-b496-f7d7ca30209a"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/"}}, "4edef67d-2227-43fc-b8b4-618a2b53af52": {"node_ids": ["ab260ffc-9379-4126-8deb-05b4d8d0a6f1"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/"}}, "10084969-a2c5-4bb5-b7d1-d8a08b05d58b": {"node_ids": ["c6cd537c-1ced-4d67-bec6-5d90bbebb76c"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/understanding/querying/querying/"}}, "8a434e7a-9abb-4432-bc6a-8621b18e9c86": {"node_ids": ["87fba420-19be-4d5b-ab4b-4622d42b6557"], "metadata": {"tag": "p", "url": "https://drive.google.com/file/u/0/d/1SPS5LeqXBBKmyQGIR2lrpeagLWwVerpU/edit"}}, "c89ded7b-5760-4b9d-ad7f-a557beed000a": {"node_ids": ["c4bf41c0-b3f7-4fbb-9105-4bd7e1126f62"], "metadata": {"tag": "p", "url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}}, "c3d02fa5-7dec-4b91-9e79-d261e0322194": {"node_ids": ["ad8a32fb-8eac-4e0d-838b-47210d45831d"], "metadata": {"tag": "p", "url": "https://github.com/deenassq/Sentiment-Analysis-for-STC-Customers-Tweets"}}, "261dd891-15e7-47f7-9fe3-3d73179e2b35": {"node_ids": ["95b002f8-bf16-40b2-8b77-ee96af602c73"], "metadata": {"tag": "p", "url": "https://github.com/kv-22"}}, "5ebfe72f-ec7b-4c88-bd2c-da17f5acf7f2": {"node_ids": ["218b28bb-9ae4-4cf9-a750-f88b188f4308"], "metadata": {"tag": "p", "url": "https://github.com/kv-22/chatWithHistory"}}, "063555a2-c6a8-4ba6-8d8b-d52137e29ace": {"node_ids": ["f7b1b69a-ca6d-4ddd-bd61-3fc532ac311b"], "metadata": {"tag": "p", "url": "https://github.com/kv-22/chatWithHistory/tree/main"}}, "7b3511f5-b591-4d43-a74f-c21386aa80db": {"node_ids": ["552bf27e-bce5-40bb-9d34-a3bede7a748b"], "metadata": {"tag": "p", "url": "https://github.com/kv-22?tab=repositories"}}, "a47ab726-a49c-4ea5-925c-e1df812cd914": {"node_ids": ["48a1e406-c8df-4700-bd1a-bd73a504c807"], "metadata": {"tag": "p", "url": "https://github.com/predlico/ARAGOG"}}, "f1b72519-0c6b-4d24-969e-83e11bb57508": {"node_ids": ["482c8e4a-90fa-4d96-bf1f-2f212921eea5"], "metadata": {"tag": "p", "url": "https://github.com/run-llama/llama_index/issues/3928"}}, "caa9fdad-b18c-478e-b162-b89e271ee36e": {"node_ids": ["82ccf39b-6225-4a9e-8f1e-0040f45be498"], "metadata": {"tag": "p", "url": "https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488"}}, "56cd6d27-ff55-45dd-96fb-73f90ff73476": {"node_ids": ["2368581c-b2cc-4c46-8ef4-cf051fd7fcc5"], "metadata": {"tag": "p", "url": "https://repair2care.com/en_sa/services/exterior-repairs/alloy-wheels-repairs/"}}, "7cf9fe29-5ba3-4efb-9fea-86a6edb4b770": {"node_ids": ["a7ddc733-320f-40c7-a97a-0241900cd290"], "metadata": {"tag": "p", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html"}}, "6715c28f-5680-4da9-8800-f98f728af043": {"node_ids": ["31aaf241-ba89-460e-b8bf-6126f34d777b"], "metadata": {"tag": "p", "url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}}, "bbb0e710-d7ac-48f0-b3ff-48a16b7e0cdf": {"node_ids": ["1a4bd6df-9c8e-4c9a-bc77-6ecaa7dc40a2"], "metadata": {"tag": "p", "url": "https://www.alloywheel.sa/"}}, "ca363537-6944-40ca-bcba-412941039b40": {"node_ids": ["da43707f-ac52-44e4-af89-c812a627330b"], "metadata": {"tag": "p", "url": "https://www.allrecipes.com/recipe/10813/best-chocolate-chip-cookies/"}}, "8dcdff1e-32e9-4e81-87e0-091f47ce0e76": {"node_ids": ["ebe08b61-8b48-4a53-a48f-f1ef939e6310"], "metadata": {"tag": "p", "url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}}, "2be8f15d-c0f3-4503-be54-0d33ea4b1e11": {"node_ids": ["8d068ad3-382f-4cb9-a8ff-1561de776933"], "metadata": {"tag": "p", "url": "https://www.cobone.com/en/deals/auto-riyadh/1-rims-colour-zoom-car/126639"}}, "43a46798-f605-4dd1-b6d8-0d5da8ac6f2f": {"node_ids": ["720469a2-ba11-4992-816d-6e1a9cf1748d"], "metadata": {"tag": "p", "url": "https://www.ibm.com/topics/machine-learning"}}, "8e3f523f-46e7-4fcc-bfd0-d7eca1da0fbc": {"node_ids": ["6bf1b395-52e0-4cac-9594-95fea9af628f"], "metadata": {"tag": "p", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}}, "e20b7ead-1753-4937-b759-891824c6f8a6": {"node_ids": ["fee7fa61-ff8a-4a7b-9ad6-bedd01d90ff8"], "metadata": {"tag": "p", "url": "https://www.nytimes.com/wirecutter/reviews/best-mascara/"}}, "6b23408d-eb4c-49f9-92e1-dc29037423a3": {"node_ids": ["542345d9-9fed-4e41-9613-a177356a2630"], "metadata": {"tag": "p", "url": "https://www.rac.co.uk/drive/advice/car-maintenance/how-much-does-alloy-wheel-refurbishment-cost/"}}}}