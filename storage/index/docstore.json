{"docstore/data": {"f127012a-37c0-46ec-8435-bd9b1205098e": {"__data__": {"id_": "f127012a-37c0-46ec-8435-bd9b1205098e", "embedding": null, "metadata": {"tag": "p", "url": "https://aws.amazon.com/what-is/reinforcement-learning/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80d7e16f-0a5a-41af-b441-6a570f4d4372", "node_type": "4", "metadata": {"url": "https://aws.amazon.com/what-is/reinforcement-learning/"}, "hash": "3a32c0ba234d7d01ef8b05590e7aed58bd45b257987827129f505c475a392e75", "class_name": "RelatedNodeInfo"}}, "text": "We use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can click \u201cCustomize cookies\u201d to decline performance cookies. If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To continue without accepting these cookies, click \u201cContinue without accepting.\u201d To make more detailed choices or learn more, click \u201cCustomize cookies.\u201d Essential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. Performance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes. Functional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly. Advertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising. Blocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by clicking Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the AWS Cookie Notice. We will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists. Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want \u00bb Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored. RL algorithms use a reward-and-punishment paradigm as they process data. They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes. The algorithms are also capable of delayed gratification. The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments. There are many benefits to using reinforcement learning (RL). However, these three often stand out. RL algorithms can be used in complex environments with many rules and dependencies. In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment. Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results. In traditional ML algorithms, humans must label data pairs to direct the algorithm. When you use an RL algorithm, this isn\u2019t necessary. It learns by itself. At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections. RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences. It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards. For example, decisions about energy consumption or storage might have long-term consequences. RL can be used to optimize long-term energy efficiency and cost. With appropriate architectures, RL agents can also generalize their learned strategies across similar but not identical tasks. Reinforcement learning (RL) can be applied to a wide range of real-world use cases. We give some examples next. In applications like recommendation systems, RL can customize suggestions to individual users based on their interactions. This leads to more personalized experiences. For example, an application may display ads to a user based on some demographic information. With each ad interaction, the application learns which ads to display to the user to optimize product sales. Traditional optimization methods solve problems by evaluating and comparing possible solutions based on certain criteria. In contrast, RL introduces learning from interactions to find the best or close-to-best solutions over time. For example, a cloud spend optimizing system uses RL to adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations. It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization. The dynamics of financial markets are complex, with statistical properties that change over time. RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts. For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards. It dynamically creates a value function and develops a strategy to maximize profits. The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology. For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell. Soon, the child learns which combination of activities results in the end reward. An RL algorithm mimics a similar learning process. It tries different activities to learn the associated negative and positive values to achieve the end reward outcome. In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps. At every step, the agent takes a new action that results in a new environment state. Similarly, the current state is attributed to the sequence of previous actions. Through trial and error in moving through the environment, the agent builds a set of if-then rules or policies. The policies help it decide which action to take next for optimal cumulative reward. The agent must also choose between further environment exploration to learn new state-action rewards or select known high-reward actions from a given state. This is called the exploration-exploitation trade-off . There are various algorithms used in reinforcement learning (RL)\u2014such as Q-learning, policy gradient methods, Monte Carlo methods, and temporal difference learning. Deep RL is the application of deep neural networks to reinforcement learning. One example of a deep RL algorithm is Trust Region Policy Optimization (TRPO). All these algorithms can be grouped into two broad categories. Model-based RL is typically used when environments are well-defined and unchanging and where real-world environment testing is difficult. The agent first builds an internal representation (model) of the environment. It uses this process to build this model: Once the model is complete, the agent simulates action sequences based on the probability of optimal cumulative rewards. It then further assigns values to the action sequences themselves. The agent thus develops different strategies within the environment to achieve the desired end goal. Consider a robot learning to navigate a new building to reach a specific room. Initially, the robot explores freely and builds an internal model (or map) of the building. For instance, it might learn that it encounters an elevator after moving forward 10 meters from the main entrance. Once it builds the map, it can build a series of shortest-path sequences between different locations it visits frequently in the building. Model-free RL is best to use when the environment is large, complex, and not easily describable. It\u2019s also ideal when the environment is unknown and changing, and environment-based testing does not come with significant downsides. The agent doesn\u2019t build an internal model of the environment and its dynamics. Instead, it uses a trial-and-error approach within the environment. It scores and notes state-action pairs\u2014and sequences of state-action pairs\u2014to develop a policy. Consider a self-driving car that needs to navigate city traffic. Roads, traffic patterns, pedestrian behavior, and countless other factors can make the environment highly dynamic and complex. AI teams train the vehicle in a simulated environment in the initial stages. The vehicle takes actions based on its current state and receives rewards or penalties. Over time, by driving millions of miles in different virtual scenarios, the vehicle learns which actions are best for each state without explicitly modeling the entire traffic dynamics. When introduced in the real world, the vehicle uses the learned policy but continues to refine it with new data. While supervised learning, unsupervised learning, and reinforcement learning (RL) are all ML algorithms in the field of AI, there are distinctions between the three. Read about supervised and unsupervised learning \u00bb In supervised learning, you define both the input and the expected associated output. For instance, you can provide a set of images labeled dogs or cats, and the algorithm is then expected to identify a new animal image as a dog or cat. Supervised learning algorithms learn patterns and relationships between the input and output pairs. Then, they predict outcomes based on new input data. It requires a supervisor, typically a human, to label each data record in a training data set with an output. In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance. During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes. By rewarding desired behaviors, you give weightage to the best outcomes. Unsupervised learning algorithms receive inputs with no specified outputs during the training process. They find hidden patterns and relationships within the data using statistical means. For instance, you could provide a set of documents, and the algorithm may group them into categories it identifies based on the words in the text. You do not get any specific outcomes; they fall within a range. Conversely, RL has a predetermined end goal. While it takes an exploratory approach, the explorations are continuously validated and improved to increase the probability of reaching the end goal. It can teach itself to reach very specific outcomes. While reinforcement learning (RL) applications can potentially change the world, it may not be easy to deploy these algorithms. Experimenting with real-world reward and punishment systems may not be practical. For instance, testing a drone in the real world without testing in a simulator first would lead to significant numbers of broken aircraft. Real-world environments change often, significantly, and with limited warning. It can make it harder for the algorithm to be effective in practice. Like any field of science, data science also looks at conclusive research and findings to establish standards and procedures. Data scientists prefer knowing how a specific conclusion was reached for provability and replication. With complex RL algorithms, the reasons why a particular sequence of steps was taken may be difficult to ascertain. Which actions in a sequence were the ones that led to the optimal end result? This can be difficult to deduce, which causes implementation challenges. Amazon Web Services (AWS) has many offerings that help you develop, train, and deploy reinforcement learning (RL) algorithms for real-world applications. With Amazon SageMaker , developers and data scientists can quickly and easily develop scalable RL models. Combine a deep learning framework (like TensorFlow or Apache MXNet), an RL toolkit (like RL Coach or RLlib), and an environment to mimic a real-world scenario. You can use it to create and test your model. With AWS RoboMaker , developers can run, scale, and automate simulation with RL algorithms for robotics without any infrastructure requirements. Get hands-on experience with AWS DeepRacer , the fully autonomous 1/18th scale race car. It boasts a fully configured cloud environment that you can use to train your RL models and neural network configurations. Get started with reinforcement learning on AWS by creating an account today.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "702e865c-3f65-4cc7-85af-7ef9d203180e": {"__data__": {"id_": "702e865c-3f65-4cc7-85af-7ef9d203180e", "embedding": null, "metadata": {"tag": "p", "url": "https://builtin.com/machine-learning/machine-learning-basics"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11010739-0863-47eb-a7c9-a0c44cbac59f", "node_type": "4", "metadata": {"url": "https://builtin.com/machine-learning/machine-learning-basics"}, "hash": "1ce19583dcbef156ea67066410906cee37626e3c553a700403b55dbe2dadedbb", "class_name": "RelatedNodeInfo"}}, "text": "Machine learning is an application of artificial intelligence where a machine learns from past experiences (input data) and makes future predictions. Machine learning is an application of artificial intelligence where a machine learns from past experiences (input data) and makes future predictions. It\u2019s typically divided into three categories: supervised learning , unsupervised learning and reinforcement learning . This article introduces the basics of machine learning theory, laying down the common concepts and techniques involved. This post is intended for people starting with machine learning, making it easy to follow the core concepts and get comfortable with machine learning basics. Machine learning is an application of artificial intelligence in which a machine learns from past experiences or input data to make future predictions. There are three common categories of machine learning: supervised learning, unsupervised learning and reinforcement learning. In 1959, Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, described machine learning as \u201cthe study that gives computers the ability to learn without being explicitly programmed.\u201d Alan Turing\u2019s seminal paper introduced a benchmark standard for demonstrating machine intelligence, such that a machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being. A more technical definition given by Tom M. Mitchell\u2019s 1997 paper : \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\u201d For example, a handwriting recognition learning problem: In order to perform the task T, the system learns from the data set provided. A data set is a collection of many examples. An example is a collection of features. More on Machine LearningAll Machine Learning Models Explained Machine learning is generally categorized into three types: Supervised learning, unsupervised learning, reinforcement learning. In supervised learning the machine experiences the examples along with the labels or targets for each example. The labels in the data help the algorithm to correlate the features. Two of the most common supervised machine learning tasks are classification and regression . In classification, the machine learning model takes in data and predicts the most likely category, class or label it belongs to based on its values. Some examples of classification include predicting stock prices and categorizing articles to news, politics or leisure based on its content. In regression, the machine predicts the value of a continuous response variable. Common examples include predicting sales of a new product or a salary for a job based on its description. When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data . There is no label or target given for the examples. One common task is to group similar examples together called clustering . Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best. This is known as the reinforcement signal. For example, maximizing the points won in a game over a lot of moves. Regression is a technique used to predict the value of response (dependent) variables from one or more predictor (independent) variables. Most commonly used regressions techniques are linear regression and logistic regression . We will discuss the theory behind these two prominent techniques alongside explaining many other key concepts like gradient descent algorithm, overfit and underfit, error analysis, regularization, hyperparameters and cross validation techniques involved in machine learning. In linear regression problems, the goal is to predict a real-value variable y from a given pattern X . In the case of linear regression the output is a linear function of the input. Let y\u0302 be the output our model predicts: y\u0302 = WX+b Here X is a vector or features of an example, W are the weights or vector of parameters that determine how each feature affects the prediction, and b is a bias term. So, our task T is to predict y from X . Now ,we need to measure performance P to know how well the model performs. To calculate the performance of the model, we first calculate the error of each example i as: We then take the absolute value of the error to take into account both positive and negative values of error. Finally, we calculate the mean for all recorded absolute errors or the average sum of all absolute errors. Mean absolute error (MAE) equals the average of all absolute errors: A more popular way of measuring model performance is using Mean squared error (MSE) . This is the average of squared differences between prediction and actual observation. The mean is halved as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the half term. The main aim of training the machine learning algorithm is to adjust the weights W to reduce the MAE or MSE. To minimize the error, the model updates the model parameters W while experiencing the examples of the training set. These error calculations when plotted against the W is also called cost function J(w) , since it determines the cost/penalty of the model. So, minimizing the error is also called as minimizing the cost function J . In the gradient descent algorithm , we start with random model parameters and calculate the error for each learning iteration, keep updating the model parameters to move closer to the values that results in minimum cost. Repeat until minimum cost: In the above equation, we are updating the model parameters after each iteration. The second term of the equation calculates the slope or gradient of the curve at each iteration. The gradient of the cost function is calculated as a partial derivative of cost function J with respect to each model parameter wj , where j takes the value of number of features [1 to n] . \u03b1 , alpha, is the learning rate, or how quickly we want to move towards the minimum. If \u03b1 is too large, we can overshoot. If \u03b1 is too small, it means small steps of learning, which increases the overall time it takes the model to observe all examples. There are three ways of doing gradient descent: In some problems the response variable isn\u2019t normally distributed . For instance, a coin toss can result in two outcomes: heads or tails. The Bernoulli distribution describes the probability distribution of a random variable that can take the positive case with probability P or the negative case with probability 1-P . If the response variable represents a probability, it must be constrained to the range {0,1} . In logistic regression, the response variable describes the probability that the outcome is the positive case. If the response variable is equal to or exceeds a discrimination threshold, the positive class is predicted. Otherwise, the negative class is predicted. The response variable is modeled as a function of a linear combination of the input variables using the logistic function. Since our hypotheses y\u0302 has to satisfy 0 \u2264 y\u0302 \u2264 1 , this can be accomplished by plugging logistic function or sigmoid function : The function g(z) maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification . Now, coming back to our logistic regression problem, let\u2019s assume that z is a linear function of a single explanatory variable x . We can then express z as follows: And the logistic function can now be written as: g(x) is interpreted as the probability of the dependent variable. g(x) = 0.7 , gives us a probability of 70 percent that our output is one. Our probability that our prediction is zero is just the complement of our probability that it is one. For example, if the probability that it\u2019s one is 70 percent, then the probability that it is zero is 30 percent. The input to the sigmoid function g doesn\u2019t need to be a linear function. It can be a circle or any shape. We cannot use the same cost function that we used for linear regression because the sigmoid function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. In order to ensure the cost function is convex, and therefore, ensure convergence to the global minimum, the cost function is transformed using the logarithm of the sigmoid function. The cost function for logistic regression looks like: Which can be written as: So, the cost function for logistic regression is: Since the cost function is a convex function, we can run the gradient descent algorithm to find the minimum cost. We try to make the machine learning algorithm fit the input data by increasing or decreasing the model\u2019s capacity. In linear regression problems, we increase or decrease the degree of the polynomials. Consider the problem of predicting y from x \u2208 R . Since the data doesn\u2019t lie in a straight line, the fit is not very good. To increase model capacity, we add another feature by adding the term x\u00b2 to it. This produces a better fit. But if we keep on doing so x\u2075 , fifth order polynomial), we may be able to better fit the data but it will not generalize well for new data. When the model has fewer features, it isn\u2019t able to learn from the data very well. This means the model has a high bias. When the model has complex functions, it\u2019s able to fit the data but is not able to generalize to predict new data. This model has high variance. There are three main options to address the issue of overfitting: Regularization can be applied to both linear and logistic regression by adding a penalty term to the error function in order to discourage the coefficients or weights from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified linear regression error function: Where lambda is our regularization parameter. In order to minimize the error, we use the gradient descent algorithm. We keep updating the model parameters to move closer to the values that result in minimum cost. Then repeat until convergence, with regularization: With some manipulation, the above equation can also be represented as: The first term in the above equation will always be less than one: You can see it as reducing the value of the coefficient by some amount on every update. The cost function of the logistic regression with regularization is: Then, repeat until convergence with regularization: The regularization term used in the previous equations is called L2 , or ridge regularization. The L2 penalty aims to minimize the squared magnitude of the weights. There is another regularization called L1, or lasso: The L1 penalty aims to minimize the absolute value of the weights. Hyperparameters are higher-level parameters that describe structural information about a model that must be decided before fitting model parameters. Examples of hyperparameters we discussed so far include: Learning rate (alpha ) and regularization (lambda ). The process to select the optimal values of hyperparameters is called model selection . if we reuse the same test data set over and over again during model selection, it will become part of our training data, and the model will be more likely to over fit. The overall data set is divided into three categories: The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of keeping a test set that the model hasn\u2019t seen before during the training and model selection steps is to avoid overfitting the model. The model is able to better generalize to unseen data. In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is to use cross-validation. More on Machine LearningUnderstanding Feature Importance in Machine Learning These are the steps for selecting hyper-parameters using K-fold cross-validation: Cross-validation allows us to tune hyperparameters with only our training set. This allows us to keep the test set as a truly unseen data set for selecting the final model. We\u2019ve covered some of the key concepts in the field of machine learning, starting with the definition of machine learning and then covering different types of machine learning techniques. We discussed the theory behind the most common regression techniques (linear and logistic) alongside other key concepts of machine learning.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fb806a4-5df1-42b6-b8bc-d209d471ac06": {"__data__": {"id_": "0fb806a4-5df1-42b6-b8bc-d209d471ac06", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76515f67-dfd3-4a2f-bfc6-865c8f94be45", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/"}, "hash": "0f52145bb7bcfdbd32c4e5b7db160c57705951355ddc7ebcbd6cc6a7200ce49a", "class_name": "RelatedNodeInfo"}}, "text": "Embeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search! At a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs. When calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings. There are many embedding models to pick from. By default, LlamaIndex uses text-embedding-ada-002 from OpenAI. We also support any embedding model offered by Langchain here , as well as providing an easy to extend base class for implementing your own embeddings. Most commonly in LlamaIndex, embedding models will be specified in the Settings object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on. You can also specify embedding models per-index. If you don't already have your embeddings installed: Then: To save costs, you may want to use a local model. This will use a well-performing and fast default from Hugging Face. You can find more usage details and available customization options below. The most common usage for an embedding model will be setting it in the global Settings object, and then using it to construct an index and query. The input documents will be broken into nodes, and the embedding model will generate an embedding for each node. By default, LlamaIndex will use text-embedding-ada-002 , which is what the example below manually sets up for you. Then, at query time, the embedding model will be used again to embed the query text. By default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small. The easiest way to use a local model is: LlamaIndex also supports creating and using ONNX embeddings using the Optimum library from HuggingFace. Simple create and save the ONNX embeddings, and use them. Some prerequisites: Creation with specifying the model and output path: And then usage: We also support any embeddings offered by Langchain here . The example below loads a model from Hugging Face, using Langchain's embedding class. If you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own! The example below uses Instructor Embeddings ( install/setup details here ), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic. You can also use embeddings as a standalone module for your project, existing application, or general testing and exploration. We support integrations with OpenAI, Azure, and anything LangChain offers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dca539e-49a0-44df-bc14-e33a85c64dbd": {"__data__": {"id_": "6dca539e-49a0-44df-bc14-e33a85c64dbd", "embedding": null, "metadata": {"tag": "p", "url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b2e4b53-1ffe-4901-ab94-2d4e070e57a3", "node_type": "4", "metadata": {"url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}, "hash": "8eafeaa66fb770347709f5a1b3178a3e2ba48c815541e9db7425b331b8950d61", "class_name": "RelatedNodeInfo"}}, "text": "\u00a9 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.\ud83d\ude80 Welcome to our AI-Generated Text Detection project! In this repository, we present a robust solution for detecting AI-generated text using BERT, a cutting-edge natural language processing model. Whether you're a researcher, developer, or a curious enthusiast, this project empowers you to explore, understand, and combat AI-generated content effectively. AI-generated content is becoming increasingly sophisticated, making it challenging to distinguish between genuine and computer-generated text. Our project aims to tackle this issue by leveraging the power of BERT (Bidirectional Encoder Representations from Transformers) to identify and flag AI-generated text segments. Whether you're dealing with chatbots, articles, or social media posts, our solution offers accurate detection, ensuring the authenticity of digital content. Follow these simple steps to get started with our AI-Generated Text Detection tool: Our solution follows a comprehensive approach to AI-generated text detection: Data Preprocessing: We clean and preprocess the textual data, removing noise and irrelevant information to enhance the accuracy of our model. BERT Tokenization: Leveraging the BERT tokenizer, we encode the preprocessed text, preparing it for input into our detection model. Model Training: Using a BERT-based sequence classification model, we train the system to distinguish between genuine and AI-generated text with a high degree of accuracy. Predictions: Once trained, the model generates predictions for test data, highlighting potential AI-generated content segments. Result Analysis: The results are saved in a CSV file, allowing users to review and analyze the detected segments along with their confidence scores. We welcome contributions from the community! Whether you're a seasoned developer, a data science enthusiast, or a domain expert, your insights and expertise can enhance our project. \ud83d\ude80 Connect With Me: If you find this project interesting or helpful, don't hesitate to follow me for more exciting updates and projects! Let's learn and grow together! \ud83c\udf1f AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.\ud83d\ude80", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d61ffe15-bd0e-4517-9a4a-2d8e16ef6a42": {"__data__": {"id_": "d61ffe15-bd0e-4517-9a4a-2d8e16ef6a42", "embedding": null, "metadata": {"tag": "p", "url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea5748e7-c7d1-43aa-a80b-15914273c94b", "node_type": "4", "metadata": {"url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca"}, "hash": "a933f6261a25947b48cf52a3d56de86de07b6c53bfc160ac32e5576f27ef6afe", "class_name": "RelatedNodeInfo"}}, "text": "Sign up Sign in Sign up Sign in Dipika Baad Follow The Startup -- 1 Listen Share Background to Word Embeddings and Implementing Sentiment Classification on Yelp Restaurant Review Text Data using Word2Vec. How the word embeddings are learned and used for different tasks will be explored in the beginning followed by using Word2Vec vectors for doing sentiment classification on Yelp Restaurant Review Dataset. In my previous posts of Sentiment Classification using BOW and Sentiment Classication using TFIDF , I have covered the topics of preprocessing the text and loading the data. This will be similar to those posts and we can quickly compare the results to those at the end as well. This post is one step further where little more complex method for representing the text is used to get the vectors for documents which tries to capture more than just the word information/importance. Let\u2019s begin with loading the data. Yelp restaurant review dataset can be downloaded from their site and the format of the data present there is JSON. The data provided is actually not in correct json format readable for python. Each row is dictionary but for it to be a valid json format, a square bracket should be at the start and end of the file with , being added at end of each row. Define the INPUT_FOLDER as folder path in your local directory where yelp review.json file is present. Declare OUTPUT_FOLDER as a path where you want to write the output from the following function. Loading of json data and writing the top 100,000 rows is done in the following function: Once the above function has been run, you are ready to load it in pandas dataframe for the next steps. For the experiment, only small amount of data is taken so that it can be run faster to see the results. After the data is loaded, new column for sentiment indication is created. It is not always the situation that some column with the prediction label you want to do is present in the original dataset. This can be a derived column in most of the cases. For this case, stars column in the data is used to derive sentiment. Output: After the data is available, mapping from stars to sentiment is done and distribution for each sentiment is plotted. Output: Once that is done, number of rows for each sentiment is checked. Sentiment Classes are as follows: Number of rows are not equally distributed across these three sentiments. In this post, problem of imbalanced classes won\u2019t be dealt that is why, simple function to retrieve the top few records for each sentiment is written. In this example, top_n is 10000 which means total of 30,000 records will be taken. Output: Preprocessing involves many steps like tokenization, removing stop words, stemming/lemmatization etc. These commonly used techniques were explained in detail in my previous post of BOW . Here, only the necessary steps are explained in the next phase. Why do you need to preprocess this text? \u2014 Not all the information is useful in making predictions or doing classifications. Reducing the number of words will reduce the input dimension to your model. The way the language is written, it contains lot of information which is grammar specific. Thus when converting to numeric format, word specific characteristics like capitalisation, punctuations, suffixes/prefixes etc. are redundant. Cleaning the data in a way that similar words map to single word and removing the grammar relevant information from text can tremendously reduce the vocabulary. Which methods to apply and which ones to skip depends on the problem at hand. Stop words are the words which are commonly used and removed from the sentence as pre-step in different Natural Language Processing (NLP) tasks. Example of stop words are: \u2018a\u2019, \u2018an\u2019, \u2018the\u2019, \u2018this\u2019, \u2018not\u2019 etc. Every tool uses a bit different set of stop words list that it removes but this technique is avoided in cases where phrase structure matters like in this case of Sentiment Analysis. Example of removing stop words: Output: As it can be seen from the output, removal of stop words removes necessary words required to get the sentiment and sometimes it can totally change the meaning of the sentence. In the examples printed by above piece of code, it is clear that it can convert a negative statement into positive sentence. Thus, this step is skipped for Sentiment Classification. Tokenization is the process in which the sentence/text is split into array of words called tokens. This helps to do transformations on each words separately and this is also required to transform words to numbers. There are different ways of performing tokenization. I have explained these ways in my previous post under Tokenization section, so if you are interested you can check it out. Gensim\u2019s simple_preprocess allows you to convert text to lower case and remove punctuations. It has min and max length parameters as well which help to filter out rare words and most commonly words which will fall in that range of lengths. Here, simple_preprocess is used to get the tokens for the dataframe as it does most of the preprocessing already for us. Let\u2019s apply this method to get the tokens for the dataframe: Output: 3. Stemming Stemming process reduces the words to its\u2019 root word. Unlike Lemmatization which uses grammar rules and dictionary for mapping words to root form, stemming simply removes suffixes/prefixes. Stemming is widely used in the application of SEOs, Web search results, and information retrieval since as long as the root matches in the text somewhere it helps to retrieve all the related documents in the search. There are different algorithms used to do the stemming. PorterStammer(1979), LancasterStammer (1990), and SnowballStemmer ( can add custom rules). NLTK or Gensim package can be used for implementing these algorithms for stemming. Lancaster is bit slower than Porter so we can use it according to size and response time required. Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. It is not very clear which one will produce accurate results, so one has to experiment different methods and choose the one that gives better results. In this example, Porter Stemmer is used which is simple and speedy. Following code shows how to implement stemming on dataframe and new column stemmed_tokens is created: Output: Train data would be used to train the model and test data is the data on which the model would predict the classes and it will be compared with original labels to check the accuracy or other model test metrics. Try to balance the number of classes in both the sets so that the results are not biased or one of the reasons for insufficient model training. This is a crucial part of machine learning model. In real-world problems, there are cases of imbalanced classes which needs using techniques like oversampling minority class, undersampling majority class ( Resample function from scikit-learn packaged or generating synthetic samples using SMOTE functionality in Imblearn package . For this case, the data is split into two parts, train and test with 70% in train and 30% in test. While making the splitting, it is better to have equal distribution of classes in both train and test data. Here, function train_test_split from scikit-learn package is used. Output: As it can be seen from the above output, data is distributed for each classes proportionately. Number of rows for each sentiment in train and test are printed. Word embeddings are words mapped to real number vectors such that it can capture the semantic meaning of words. The methods tried in my previous posts of BOW and TFIDF do not capture the meaning between the words, they consider the words seperately as features. Word embeddings use some models to map a word into vectors such that similar words will be closer to each other. As shown in the below figure, for example some of the positive words which are adjectives will be closer to each other and vice versa for negative adjectives. It captures semantical and syntactical information of words. To train this model it takes into consideration the words surrounding that word of particular window size. There are different ways of deriving the word embedding vectors. Word2vec is one such method where neural embeddings model is used to learn that. It uses following two architectures to achieve this. Here the model predicts the word under consideration given context words within specific window. The hidden layer has the number of dimensions in which the current word needs to be represented at the output layer. Following diagram shows as example with window of size 2 for predicting vector for word \u2018awesome\u2019 given a sentence \u2018Restaurant was awesome this time\u2019. Skip gram is opposite of CBOW where it predicts embeddings for the surrounding context words in the specific window given a current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer. Following shows an example with window size of 2. In this case, we will be using gensim\u2019s Word2Vec for creating the model. Some of the important parameters are as follows: Output: It is usually better to save the model in some file so you don\u2019t have to rerun it every time doing the training for the classifier. In the next part, we will reload the model and see how to access the word2vec dictionary as well. Output: Next will see how to use the Word2Vec model to get the vector for documents in the dataset. Word2Vec vectors are generated for each review in train data by traversing through the X_train dataset. By simply using the model on each word of the review, we get the word embedding vectors for those words. We will be implementing average over all the vectors of words in a sentence and that will represent a sentence from our dataset. These vectors are stored in a csv file. You can directly create this in a dataframe but when there is a large amount of data it is better to write to a file as and when the vector is created and if the code breaks you can start from the point where it had broken. Following code, writes the vectors in the OUTPUT_FOLDER defined in the first step. Once the Word2Vec vectors are ready for training, we load it in dataframe. DecisionTreeClassifier is used here to do the sentiment classification. Decision tree classifier is Supervised Machine learning algorithm for classification. In this example, scikit-learn package is used for implementing the decision tree classifier class . The fit function is used to fit the input feature vectors against the sentiments in train data. Following code shows how to train the classifier with Word2Vec vectors. Output: This took ~36 seconds to train for our input data. clf_decision_word2vec variable can be now used to do the predictions. Time to see how the model worked out at the end of this all facade. Output: Classification Report shows the average accuracy which is 0.52 . This is a good result compared to the amount of data used for training. The predict function can be used on the model object to get the predicted class for the test data. Accuracy for positive and negative sentiments is better than neutral which makes sense as it is hard to distinguish the neutral comments compared to commonly used words in the positive and negative sentiment. This accuracy is little bit less compared to BOW Classification and TFIDF Classification done in previous posts. One thing to notice is that the total input dimension has reduced from vocab size of 30056 to 1000 in case of Word2Vec. That is here the dimension can be made custom of less size but as it is capturing the necessary things and only limited things to describe the words it is a good compromise between accuracy and computational complexity for the classification model. So now you can easily experiment with your own dataset with this method! I hope this helped you to understand how to use Word2Vec vectors to do the sentiment analysis on restaurant reviews data. Feel free to extend this code! This is applicable to any other text classification problems where multiple classes are there. If I can think about improving this model, I would use different hyper-parameters for decision classifier or even try out other classification models. Input parameters like size , min_count and window_size of word2vec function can be experimented to get a better accuracy than this. Instead of using average, min and max of word vectors of words in a sentence can be taken as well. Preprocessing can be changed to use lemmatization or other stemming algorithms to see how the results change. As always \u2014 Happy experimenting and learning :) -- -- 1 The Startup Big Data Consultant @Netlight | CoFounder @HuskyCodes | Web developer | Passionate about coding, dancing, reading Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ede74a3d-04ea-40f5-b952-91deff0339ba": {"__data__": {"id_": "ede74a3d-04ea-40f5-b952-91deff0339ba", "embedding": null, "metadata": {"tag": "p", "url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b06fa33a-c204-4841-a05f-dbdb74376131", "node_type": "4", "metadata": {"url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}, "hash": "32309870fddf919749ab74a489acd217f9e549812b6af1482b360ba02cd3618f", "class_name": "RelatedNodeInfo"}}, "text": "Have a recipe of your own to share? Submit your recipe There are a few crucial steps to baking the chewiest, tastiest chocolate chip cookies. First, skip using chips and opt for chunks. Second, instead of using just one type of chocolate (like semisweet), use a mix of semisweet, milk, and dark chocolate. Third, take the time to allow the cookie dough to rest overnight in the refrigerator. Yes, we know it's a pain, but doing so will yield a cookie with a more complex flavor and delicious toffee notes. Lastly, use an ice cream scoop to portion the cookie dough onto the baking sheet. This will produce even-sized cookies every single time. Follow these simple steps, and you'll be rewarded with a cookie that's crisp and chewy on the outside and gooey on the inside! 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min Inspired by buzzfeed.com 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min for 12 cookies Estimated values based on one serving size. Build your cart with Tasty, then choose how you want to get your order from Walmart. Inspired by buzzfeed.com", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bacb0cf0-3171-473f-8bc2-22278fabddb7": {"__data__": {"id_": "bacb0cf0-3171-473f-8bc2-22278fabddb7", "embedding": null, "metadata": {"tag": "p", "url": "https://www.atlassian.com/blog/productivity/simple-ways-to-be-productive-at-work"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5570f87e-f64a-4a32-a271-a2fa2f7bdbad", "node_type": "4", "metadata": {"url": "https://www.atlassian.com/blog/productivity/simple-ways-to-be-productive-at-work"}, "hash": "14ed6fa5711e2abfd92d7bf7b7165a6f3fac97098143dbafdf7571955c31b19a", "class_name": "RelatedNodeInfo"}}, "text": "Atlassian Work Life is Atlassian\u2019s flagship publication dedicated to unleashing the potential of every team through real-life advice, inspiring stories, and thoughtful perspectives from leaders around the world. Contributing Writer Work Futurist Senior Quantitative Researcher, People Insights Contributing Writer Principal Writer Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Our State of Teams 2024 report is live! Check it out here . Use these powerful strategies to get more done (while stressing less). Contributing Writer Get stories like this in your inbox You reach the end of your workday and glance down at your (now coffee-stained) to-do list. You\u2019re immediately overcome with frustration as you realize that barely half the tasks are checked off. Seriously, what happened? You\u2019ve been at your desk for the better part of eight hours. Why didn\u2019t you get more accomplished? Maintainin a high level of productivity is not intuitive or easy \u2013 but you don\u2019t have to resign yourself to feeling discouraged and depleted at the end of every workday. We rounded up ten top-notch, rock-solid tips you can put into play to channel your focus, and defeat your to-do list \u2013 plus a quiz to help get you started. \u201cPeople naturally have ebbs and flows in their work processes or in how well they can focus,\u201d explains Dr. Melissa Gratias, a workplace productivity coach and speaker. These peaks and valleys in your focus and motivation are naturally occurring in your body, driven by your ultradian rhythms . You can\u2019t compete with science. So rather than doubling down on your caffeine intake, the smarter move is to pay close attention to the times of day when you feel most energized. Keep a journal for at least a week or two (one day isn\u2019t long enough to identify trends) and note how you feel. You\u2019ll have an easier time spotting your biological prime time \u2013 the times you\u2019re most \u201cin the zone.\u201d With that information, you can allocate your work more effectively . Plan your deep, complex, or creative work for your golden hours (you can even block off your calendar) and save menial tasks for the times when you feel a little more drained. \u201cThe critical behavior that I advise people not to fall into is setting yourself up for failure before the day even begins,\u201d Gratias says. \u201cIf we pull out a task list of 25 things to do, we\u2019re guaranteed that we\u2019re going to be disappointed in our progress at the end of the day.\u201d Try whittling down to your priorities \u2013 Gratias recommends choosing between five and nine tasks you want to make progress on that day. This ties back to a psychological principle called \u201cThe Magical Number Seven, Plus or Minus Two.\u201d Other people swear by the similar 1-3-5 rule for an empowering to-do list. Pick one big thing you need to accomplish that day, three medium things, and five little things. If you\u2019re really struggling to figure out what deserves some real estate on your list, use an Eisenhower Matrix (sometimes called a prioritization matrix) to sort through your tasks and determine which ones deserve top billing \u2013 and which ones can be delegated or fall off your list entirely. \u201cDistractions and interruptions are for sure an impediment to productivity,\u201d says Gratias. She explains that these distractions fall into two different categories: And it\u2019s not only the distraction itself that robs you of your time \u2013 it\u2019s also the time and energy you have to spend refocusing (which research has shown can take upwards of 23 minutes). While Gratias cautions that you\u2019ll never completely eliminate distractions, you can reduce them by: \u201c\u2018I am a great multitasker!\u2019 That\u2019s my favorite misconception that I hear from employees,\u201d explains Dr. Larry Rosen, Professor Emeritus and former Chair of Psychology at California State University Dominguez Hills. Here\u2019s the thing: Research shows that the human brain is actually incapable of multitasking. Instead, you\u2019re rapidly switching between different tasks \u2013 appropriately referred to as \u201ctask switching\u201d or \u201c context switching .\u201d You\u2019re basically interrupting yourself, and you know now that those self-imposed disruptions only tank your productivity. So how do you get your brain to do one thing at once? Say out loud the one thing you\u2019re going to work on (for example, \u201cI\u2019m going to finish this slide deck.\u201d). And that\u2019s it! It\u2019s called external self-talk , and plenty of research has shown that it can have a real impact on your behavior. Even if you\u2019re technically focusing on only one task at a time, repeatedly switching between different types of work \u2013 you answer an email then update a report then work on your slide deck then answer another email \u2013 can be mentally draining. You use different parts of your brain for different tasks, which means you\u2019re majorly straining your noggin by not having any sort of systematic approach to your work. Try batching your tasks, which is essentially grouping similar tasks together and doing them all at once. You can even try time blocking , where you set specific time windows for certain types of tasks (for example, you\u2019ll answer emails from 9am to 10am). You might still need to occasionally handle things outside of their designated time slots, but any effort to stick related tasks together will give your brain some welcomed respite. It\u2019s not just the stuff you do during the workday that impacts your productivity\u2014the things you do outside of work carry a lot of weight too. Maintaining healthy habits is a lot easier said than done, but even seemingly small changes can have a big impact on your energy levels, focus, and overall mood. Here are a few quick things to try: It\u2019s tempting to think that more time at your desk means you\u2019ll get more done. But in reality, studies show that taking regular breaks can can actually boost your productivity. If you\u2019re prone to getting sucked into your work and forgetting to step away, using a time management method like the Pomodoro Technique will ensure you get up for a five-minute break between every 25-minute work period. Want to maximize your impact in that short time away? Head outside for some fresh air. Studies show that getting out into nature can alleviate mental fatigue. That majority of us who suddenly found themselves working from home in Spring 2020 (shudder) know how less-than-ideal surroundings can affect our productivity. That\u2019s why it\u2019s well worth curating a workspace that helps you feel your most focused and motivated. That can mean something different to everybody, but here are a few suggestions: Other efforts, like relying on a password manager, creating templates, and using a centralized project management platform mean you can spend less time searching for what you need and more time focused on your actual work. When your environment can have such a big impact on your focus and productivity, it\u2019s worth trying to switch it up every now and then too. Move from your desk to answer some emails on your couch. Or bring your laptop out to your patio. Or do a few hours of work from your favorite coffee shop . Not only does this build in an extra break (you have to pause what you\u2019re doing and relocate), but it also helps you buckle down. Your brain loves novelty and releases dopamine when it\u2019s presented with something new and exciting. Dopamine isn\u2019t just a \u201cfeel good\u201d brain chemical \u2013 it\u2019s a powerful motivator too. \u201cI think one of the most insidious beliefs that limits productivity more than anything else is perfectionism,\u201d explains Gratias. This perfectionism not only leads to failure to start things because \u201cif we can\u2019t finish them perfectly, we don\u2019t even begin,\u201d but it also eventually leads to failure to finish because \u201cif it\u2019s not perfect, we keep working on it and keep tweaking it.\u201d While this desire to do spotless work is admirable, it can also significantly hinder your progress and productivity. How can you stop obsessing and focus on progress over perfection? When you have a task or a project, set a timebox (i.e. a certain span of time, such as 15 minutes or an hour) that dictates how long you\u2019ll work on that specific item. When the timebox ends, that task is as done as it\u2019s going to get for now. Not only does this tactic instill a sense of urgency (which inspires you to get moving), but it also removes some pressure \u2013 you\u2019re more focused on seeing how much progress you can make in that time period, rather than working until the entire task is completed. Most of us have a lot to do. But even with high expectations and the best intentions, it\u2019s hard to muster the motivation and make the most of your time at work. The truth is that you\u2019re human \u2013 you won\u2019t operate at peak efficiency all day every day. But there are still plenty of things you can try to boost your productivity levels and transform that end-of-day glance at your (reasonable) to-do list from disheartening to gratifying. Get stories like this in your inbox Kat Boogaard Contributing Writer How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections \u2013 and have fun along the way. How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections \u2013 and have fun along the way. How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections \u2013 and have fun along the way. By Atlassian Culture, tech, teams, and tips, delivered twice a month These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information. These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising. These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e91ebcc4-506c-440b-a939-83d983bc7fd3": {"__data__": {"id_": "e91ebcc4-506c-440b-a939-83d983bc7fd3", "embedding": null, "metadata": {"tag": "p", "url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0606614c-bddd-4abf-836d-faec2577f112", "node_type": "4", "metadata": {"url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}, "hash": "e5a4b5897679ee12f5b6a7ee2037cd009e3f16d7c4713b4005473648158dbc6a", "class_name": "RelatedNodeInfo"}}, "text": "We can almost guarantee you won\u2019t have leftovers\u2014this is the best chocolate chip cookie recipe around! If you do end up with any extra chocolate chip cookies, they can easily be stored for later enjoyment. To keep your leftover cookies chewy and soft, store them at room temperature in resealable food-storage plastic bags or stacked in tightly covered containers. If you want to freeze your homemade chocolate chip cookies instead, wait until they\u2019ve cooled completely. Stack them in between layers of wax paper in a covered freezer container or plastic freezer bag and freeze your cookies for up to 2 months. To thaw, keep them covered at room temperature for 1 to 2 hours. Go ahead and chow down! Want to switch it up? This chocolate chip cookie recipe can easily be customized with endless flavor combinations! Just replace the chocolate chips and nuts with the same amount of the new ingredients you want to add. Try adding macadamia nuts and white vanilla baking chips, or pack a double punch of peanut flavor with peanut butter chips and chopped peanuts. Make salted butterscotch-pecan cookies by swapping in butterscotch chips and chopped pecans, and then sprinkling your cookies with coarse salt before baking. We also have plenty of other chocolate chip cookie recipes to try. Can\u2019t decide between cookies and brownies? Our Double Chocolate Chip Cookies , are packed full of decadent chocolate flavor. Our Oatmeal Chocolate Chip Cookies , offer next-level deliciousness, too\u2014you really can never go wrong with a classic! Baking is a science, and we can prove it! Whether you prefer a soft, chewy chocolate chip cookie or a crisp, crunchy cookie, consider the factors that make a difference in cookie structure\u2014from the ratio and type of sugars used to the amount of butter or shortening used, and how much flour is stirred in. If you like your cookies slightly crispy on the outside and chewy on the inside\u2014what many people consider the perfect texture for a chocolate chip cookie\u2014just follow this recipe exactly! Our Ultimate Chocolate Chip Cookies are truly the best chocolate chip cookies around\u2014they\u2019re called \u201cultimate\u201d for a reason. Prefer your homemade chocolate chip cookies crispy and thin? Cut out the brown sugar completely, and increase the amount of granulated sugar to 1 1/2 cups. Granulated sugar contains less moisture and helps cookies spread as they bake. After taking your chocolate chip cookies out of the oven, let them cool on a cookie sheet for five minutes. The cookies will keep baking, allowing them to crisp up further as the moisture evaporates. If you want your chocolate chip cookies airy, soft and cakey, we\u2019ve got you covered. Add in an extra egg for additional moisture and structure, and add in 2 tablespoons of milk with the egg to help soften the dough as it bakes and give the cookie a cake-like crumb. Increase the amount of flour to 3 cups to help the cookies rise (rather than spread). The perfectly cakey chocolate chip cookie is just a few steps away! Want to have a crowd-favorite cookie always at the ready? Follow this chocolate chip cookie recipe to get the perfect dough, and freeze it for later baking! Freeze individual unbaked dough balls on cookie sheets. Once the dough balls are frozen, place them in plastic freezer bags and freeze them for up to 2 months. When you\u2019re ready to bake your cookies, take the frozen cookie dough balls from the freezer and place them on a cookie sheet. Let them sit at room temperature for 15 minutes as you preheat the oven. Keep an eye on your chocolate chip cookies as they bake\u2014the bake time might need to be adjusted slightly. This hack guarantees delicious cookies in a snap!", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f8faed-a4ae-4a3b-ab66-2bc04edfe6b6": {"__data__": {"id_": "29f8faed-a4ae-4a3b-ab66-2bc04edfe6b6", "embedding": null, "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1103f70-3439-440f-ab21-2a84345dfccb", "node_type": "4", "metadata": {"url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}, "hash": "9e9c6cf33ab67a53863e8c38f9c7649908b3be999990616e2a94bb716727bff2", "class_name": "RelatedNodeInfo"}}, "text": "Alongside the rise of Large Language Models (LLM), has risen a swath of programming frameworks that aim to make it easier to build applications on top of them, such as LangChain and LlamaIndex . LlamaIndex is a programming framework that aims to simplify the development of LLM-enabled applications that leverage Retrieval Augmented Generation (RAG). A core conceptual component to understanding the mechanisms behind LlamaIndex is that of query Response Modes . LlamaIndex has 5 built-in Response Modes: compact, refine, tree_summarize, accumulation, and simple_summarize. In this article, I will: To evaluate each mode, I put them to the test in helping answer a question that has plagued the world since that fateful day in November 1963: Who shot President John F. Kennedy? I will demonstrate how each Response Mode works by asking LlamaIndex to summarize the conclusions of the \u2018 Warren Commission Report \u2019, the 250,000-word Congressional report that investigated the assassination of President Kennedy. This document far exceeds the token limit for GPT-4 and most other mainstream LLMs and requires the use of RAG techniques for an LLM to answer questions on it using only passed in contextual data (not training data). The concept of Retrieval Augmented Generation (RAG) describes an approach to allow an LLM to answer questions based on data that it wasn\u2019t originally trained on. In order to do this, an LLM must be fed this data as part of the prompt, which is generally referred to as \u2018context\u2019. However, LLMs have limited input size restrictions (also called token input size), which make it impossible and impractical to pass large datasets, such as the Warren Report, in their entirety via the prompt. Instead, with the RAG-approach, a query to an LLM is broken into two parts: a \u2018retrieval step\u2019 and then a \u2018generation step\u2019. The \u2018retrieval step\u2019 attempts to identify the portions of the original dataset that are most relevant to a user supplied query and to only pass this subset of data to an LLM, alongside the original query, as part of the \u2018generative step\u2019. Essentially, RAG is a mechanism to work within the input size restrictions of LLMs by only including in the prompt the most relevant parts of the dataset needed to answer a query. Usually, the \u2018retrieval\u2019 portion of RAG utilizes tried-and-true semantic search algorithms such as Cosine Similarity , alongside a Vector Databases to perform this step. For the purposes of this article and the testing of the Response Modes, the question I am seeking to get an answer to is: \u201cWhat are the conclusions of the Warren Report?\u201d For the purposes of this article, in the retrieval step I set my code to return the top 5 most relevant \u2018chunks\u2019 of data that relate to my original query from the Warren Report as part of the Cosine similarity algorithm. These 5 chunks of data are then passed forward to the LLM, which is where LlamaIndex Response Modes come into play. When building an LLM enabled application that utilizes RAG techniques, it is still likely that the subset of data returned as part of the retrieval step, which are normally referred to as \u2018chunks\u2019, will still be too large to fit within the input token limit for a single LLM call. Most likely, multiple calls will need to be made to the LLM in order to derive a single answer that utilizes all of the retrieved chunks. LlamaIndex Response Modes govern the various approaches that can be used to break down, sequence and combine the results of multiple LLM calls with these chunks to return a single answer to the original query. As of writing, there are 5 basic Response Modes that can be used when querying with LlamaIndex. In evaluating the 5 different Response Modes, I utilize the following frameworks and tools: For each test, I pose the same question: \u201cWhat are the main conclusions of the Warren Report regarding the assassination of President Kennedy?\u201d The following is the Python code I used to evaluate each Response Mode, this one configured to use the compact Response Mode: Note also that in my call to initialize the query_engine in LlamaIndex, I set the similarity_top_k parameter to 5, which tells LlamaIndex to return the top 5 chunks of data that are semantically similar to the query as part of the retrieval step. For all Response Modes, the same 5 chunks of text are returned from the Warren Commission Report, which are available for you to view in the table below: The Response Mode called compact is the default mode used by LlamaIndex if none is specified. The way the compact mode works is that for each chunk that is returned from the retrieval step, LlamaIndex concatenates as many of those chunks together into the largest possible string that fits into a single prompt to the GPT-4 LLM. In our example, the first 4 chunks of matched text fit into the context window for a single GPT-4 call, which means that it requires 2 LLM calls to answer our query on the conclusion of the Warren Report. The 1st call made using the compact Response Mode always uses the text_qa_template prompt: LlamaIndex then takes the answer returned to this prompt, the next concatenated set of chunks (in our case simply Chunk 5) and passes it to the LLM along with the last few sentences of Chunk 4 using the refine_template prompt: As you can see, the compact Response Mode doesn\u2019t answer the question at anything resembling a coherent answer. In fact, the LLM ends up throwing up its hands and doing a spot on rendition of a high school student fumbling their way trying to answer a question they simply have no clue about. The likely reason why the compact Response Mode was unable to answer the question is that the first 4 chunks of text from the Warren Report don\u2019t actually contain the conclusions of the report, which only appears in Chunk 5 (which oddly has a lower Cosine similarity score than the proceeding chunks). Thus, the structure of the refine_template is such that if the first set of calls has gone down the wrong path, it\u2019s difficult for the final prompt to steer the LLM back onto the right track. You can see the full-text log for both of the LLM calls made with the compact Response Mode below: The second Response Mode that LlamaIndex provides is refine . The refine mode is very similar to the compact Response Mode, except that instead of attempting to concatenate as many chunks as it can to maximize the use of the LLM token limit, LlamaIndex only include 1 chunk of retrieved data for each LLM call. Starting with the text_qa_template , LlamaIndex passes in Chunk 1 to the LLM. After that, LlamaIndex then progresses sequentially through each of the remaining chunks one at a time; with each subsequent LLM call using the refine_template to build upon the answer returned from the answer returned from the previous chunk. While the compact Response Mode tried to obfuscate it\u2019s inability to answer the question behind a wall of hand-waving text, the refine Response Mode yields a much more succinct, yet equally useless answer. Much like the compact mode, the refine is much more likely to not be able to properly answer the question when the initial chunks passed to it are less relevant. In our case, the first chunk of data from the Warren Report is the introduction and forward parts of the report which do not contain any conclusions whatsoever, which is likely why it was never able to return a proper answer to our query. You can see the full-text log for all of the LLM calls made with the refine Response Mode below: The third, and by far most effective, Response Mode is tree_summarize. The astute reader might surmise from the use of the word \u2018tree\u2019 that there is a recursive property to this response mode, and they would be correct. The tree_summarize mode in its base case makes a series of LLM calls that concatenate chunks of retrieved data so that it maximizes the input token limit for the LLM. It then takes the outputs of each of these base case responses, and then passes them together to the LLM and instructs it to derive an answer using those initial answers as context. For anyone familiar working with LangChain, the tree_summarize Response Mode is essentially the same as LangChain\u2019s MapReduceDocumentChain . In our analysis of the Warren Report, the tree_summarize mode required 3 calls to the LLM, 2 for processing the 5 chunks of data, and then 1 for combining the answers from the first 2. The prompt template used by the tree_summarize Response Mode is quite simple and the same each time, regardless if we are working at the \u2018leaf\u2019 level or higher up in the processing tree: After making a similar call that includes only Chunk 5, tree_summarize then uses the following prompt template to combine the answers: The tree_summarize response mode nails the answer and delivers a thoughtful and complete summary of the findings of the Warren Report that correctly identify Lee Harvey Oswald as the assassin, while also in the same breath, disabusing any notion of a conspiracy or second shooter. Whereas the refine and compact modes were led astray by the irrelevance of the first set of chunks they analyzed, tree_summarize mode overcomes this as it is uses a map-reduce pattern to have the LLM independently analyze each concatenated chunk of data and then in a separate prompt combine the outputs of those into a single answer. You can see the full-text log for all of the LLM calls made using the tree_summarize Response Mode below: The accumulate Response Mode is quite simple, LlamaIndex makes 1 call per retrieved chunk and then returns every \u2018non-null\u2019 answer together as an array of answers. For each of the calls that LlamaIndex makes in accumulate mode, it uses a similar template to tree_summarize : The returned result from LlamaIndex is an array of strings of length 2, containing the responses returned from the LLM for chunks 4 and 5. The answers for chunks 1,2,3 are not included in this result, because for each of those calls, the LLM returned the logical equivalent of a \u2018null\u2019 response in the form of 'The context does not provide information on the main conclusions of the Warren Report regarding the assassination of President Kenne dy.\u2019 The accumulate mode doesn\u2019t so much answer the question, but instead returns n answers to the question with each of the answers being scoped simply to the context chunk passed to the LLM in that call. It is the responsibility then of the calling application to take the list of answers to produce an actual final answer to the query. \u2018Accumulate\u2019 doesn\u2019t work well for something like the Warren Report where each chunk doesn\u2019t necessarily contain the complete information to answer the question. You can see the full-text log for all of the LLM calls made using the accumulate Response Mode below: The final LlamaIndex Response Mode is simple_summarize . This perhaps the most basic and straightforward of the response modes. In this mode, LlamaIndex truncates all text chunks so that all chunks can be concatenated and passed into the LLM in a single call. No matter how many chunks are retrieved, there will only ever be a single call made to the LLM. In our example, the prompt template that is used in simple_summarize mode looks like: Surprisingly, the answer provided by simple_summarize is not bad and almost as complete as the one provided by tree_summarize However, I would attribute this more to luck then a structural advantage of the mechanism. With the simple_summarize mode, as the number of chunks returned from the retrieval step goes up, its likely that the ultimate answer returned will decrease in quality due to the knowledge being lost in the truncated segments of each chunk. You can view the full-text log of the prompt made in the simple_summarize test here . Looking at the results of my tests, the tree_summarize Response Mode returned the most comprehensive and complete answer to the question \u201cWhat are the conclusions of the Warren Report?\u201d. The recursive, map-reduce like algorithm it employs allows it to build up the correct answer looking at all matched chunks of text from the Warren Commission Report and the responses from GPT-4 to each of them. This is not to say that tree_summarize is the only Response Mode you need when building a RAG-style application, however for the purposes of summarizing content from across a large body of text, it clearly has structural advantages that lend itself more effective than the other modes. However, it\u2019s important to understand the different motivations behind each of the other 4 Response Modes and to know which circumstances each might be the best tool for the job. With the rapidly increasing sizes of input token limits for newer LLM models, it can be argued that the need for RAG will slowly diminish. However, even in a world where LLMs are able to accept million token inputs, it will always behoove a consumer of an LLM to maximize the efficiency of the each token used in the context passed in each LLM call. As such, the LlamaIndex framework provides a very neat and consistent programming model to build RAG-style applications, certainly more so than LangChain. For those of you interested in learning more about LlamaIndex and building with it, I encourage you to visit the LlamaIndex documentation , as the team has done an excellent job of building a set of , clear and easy to work through tutorials that outline the many capabilities of LlamaIndex. Bobby Gill is the co-founder and Chief Architect of BlueLabel, an award winning digital product agency headquartered in New York. With over two decades of experience in software development, he is a seasoned full-stack engineer, software architect, and AI practitioner. Bobby currently leads the BlueLabel AI/ML practice, where he is leading a team of engineers operationalizing the transformational capabilities of generative AI within BlueLabel and for a number of enterprise clients. A Digital Transformation Agency. 4.8/5 Overall Rating \u00a9 2024 BlueLabel | All Rights Reserved \u2013 Total Rating 4.8 out of 5 based on 40+ reviews", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88c68103-12c9-48c9-bd10-8d856ad183fd": {"__data__": {"id_": "88c68103-12c9-48c9-bd10-8d856ad183fd", "embedding": null, "metadata": {"tag": "p", "url": "https://www.geeksforgeeks.org/machine-learning/"}, "excluded_embed_metadata_keys": ["urls"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96d6fa6a-3f06-4d39-b05d-60941fc72c26", "node_type": "4", "metadata": {"url": "https://www.geeksforgeeks.org/machine-learning/"}, "hash": "c079a5d1eefe89c0f6d05de4c62f3cb2a9b04bb86b0387a203a11451d59e6185", "class_name": "RelatedNodeInfo"}}, "text": "Machine Learning tutorial covers basic and advanced concepts, specially designed to cater to both students and experienced working professionals. This machine learning tutorial helps you gain a solid introduction to the fundamentals of machine learning and explore a wide range of techniques, including supervised, unsupervised, and reinforcement learning. Machine learning (ML) is a subdomain of artificial intelligence (AI) that focuses on developing systems that learn\u2014or improve performance\u2014based on the data they ingest. Artificial intelligence is a broad word that refers to systems or machines that resemble human intelligence. Machine learning and AI are frequently discussed together, and the terms are occasionally used interchangeably, although they do not signify the same thing. A crucial distinction is that, while all machine learning is AI, not all AI is machine learning. Machine Learning is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more similar to humans: The ability to learn. Machine learning is actively being used today, perhaps in many more places than one would expect. Table of Content Answer : Machine learning develop programs that can access data and learn from it. Deep learning is the sub domain of the machine learning. Deep learning supports automatic extraction of features from the raw data. Answer : Answer : Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems. Answer :", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"f127012a-37c0-46ec-8435-bd9b1205098e": {"doc_hash": "5093388b29a36b4761c3d0ee27f3d880e82cbb9c48222a50c8b6cd5eb071baaf", "ref_doc_id": "80d7e16f-0a5a-41af-b441-6a570f4d4372"}, "702e865c-3f65-4cc7-85af-7ef9d203180e": {"doc_hash": "facda6b864e0bf6907bcd12848287cbaa962f967ec9f78cc232bc4b5459d79b7", "ref_doc_id": "11010739-0863-47eb-a7c9-a0c44cbac59f"}, "0fb806a4-5df1-42b6-b8bc-d209d471ac06": {"doc_hash": "4c364b7ddedf51928f2062e8f1ced584cf332f48ae1694a23ab6fca953e3c41f", "ref_doc_id": "76515f67-dfd3-4a2f-bfc6-865c8f94be45"}, "6dca539e-49a0-44df-bc14-e33a85c64dbd": {"doc_hash": "aac9da820c9c85ee55f914763289e49d90bc13a2d56ecaa469e15c47c1a70026", "ref_doc_id": "9b2e4b53-1ffe-4901-ab94-2d4e070e57a3"}, "d61ffe15-bd0e-4517-9a4a-2d8e16ef6a42": {"doc_hash": "0cbc2843ee6d2b3acf82917d19cf2a9c8e0aa4d4fc90301fa69f74f0f6a417c8", "ref_doc_id": "ea5748e7-c7d1-43aa-a80b-15914273c94b"}, "ede74a3d-04ea-40f5-b952-91deff0339ba": {"doc_hash": "0d92ed762da054884e8b7251cb81ca0cfd02424154ea8890d6169202ad9a53a4", "ref_doc_id": "b06fa33a-c204-4841-a05f-dbdb74376131"}, "bacb0cf0-3171-473f-8bc2-22278fabddb7": {"doc_hash": "eb591c61959342647cb42f6cf8267574e0477cfd1457d218f4b87996741c1035", "ref_doc_id": "5570f87e-f64a-4a32-a271-a2fa2f7bdbad"}, "e91ebcc4-506c-440b-a939-83d983bc7fd3": {"doc_hash": "642f352df5efb490689db512d7bc7e19b44edddc020cf15e2fcadfac7818c37a", "ref_doc_id": "0606614c-bddd-4abf-836d-faec2577f112"}, "29f8faed-a4ae-4a3b-ab66-2bc04edfe6b6": {"doc_hash": "59931df934b914ba23da8a4b9be689ee88fb6f929e13d833884ed6bad7c66858", "ref_doc_id": "e1103f70-3439-440f-ab21-2a84345dfccb"}, "88c68103-12c9-48c9-bd10-8d856ad183fd": {"doc_hash": "0ff7e0bfe6f1de1dcb4056ecf42612c3842a792d9f24a8d2602d4bf909c6d97e", "ref_doc_id": "96d6fa6a-3f06-4d39-b05d-60941fc72c26"}}, "docstore/ref_doc_info": {"80d7e16f-0a5a-41af-b441-6a570f4d4372": {"node_ids": ["f127012a-37c0-46ec-8435-bd9b1205098e"], "metadata": {"tag": "p", "url": "https://aws.amazon.com/what-is/reinforcement-learning/"}}, "11010739-0863-47eb-a7c9-a0c44cbac59f": {"node_ids": ["702e865c-3f65-4cc7-85af-7ef9d203180e"], "metadata": {"tag": "p", "url": "https://builtin.com/machine-learning/machine-learning-basics"}}, "76515f67-dfd3-4a2f-bfc6-865c8f94be45": {"node_ids": ["0fb806a4-5df1-42b6-b8bc-d209d471ac06"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/"}}, "9b2e4b53-1ffe-4901-ab94-2d4e070e57a3": {"node_ids": ["6dca539e-49a0-44df-bc14-e33a85c64dbd"], "metadata": {"tag": "p", "url": "https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text"}}, "ea5748e7-c7d1-43aa-a80b-15914273c94b": {"node_ids": ["d61ffe15-bd0e-4517-9a4a-2d8e16ef6a42"], "metadata": {"tag": "p", "url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca"}}, "b06fa33a-c204-4841-a05f-dbdb74376131": {"node_ids": ["ede74a3d-04ea-40f5-b952-91deff0339ba"], "metadata": {"tag": "p", "url": "https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies"}}, "5570f87e-f64a-4a32-a271-a2fa2f7bdbad": {"node_ids": ["bacb0cf0-3171-473f-8bc2-22278fabddb7"], "metadata": {"tag": "p", "url": "https://www.atlassian.com/blog/productivity/simple-ways-to-be-productive-at-work"}}, "0606614c-bddd-4abf-836d-faec2577f112": {"node_ids": ["e91ebcc4-506c-440b-a939-83d983bc7fd3"], "metadata": {"tag": "p", "url": "https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57"}}, "e1103f70-3439-440f-ab21-2a84345dfccb": {"node_ids": ["29f8faed-a4ae-4a3b-ab66-2bc04edfe6b6"], "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}}, "96d6fa6a-3f06-4d39-b05d-60941fc72c26": {"node_ids": ["88c68103-12c9-48c9-bd10-8d856ad183fd"], "metadata": {"tag": "p", "url": "https://www.geeksforgeeks.org/machine-learning/"}}}}