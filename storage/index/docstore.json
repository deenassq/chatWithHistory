{"docstore/data": {"23529a5c-c600-4b4f-875d-1741a9dcddff": {"__data__": {"id_": "23529a5c-c600-4b4f-875d-1741a9dcddff", "embedding": null, "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/understanding/storing/storing/"}, "excluded_embed_metadata_keys": ["url"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "531b7d2d-ce99-4996-85d5-a8ce66b75f3f", "node_type": "4", "metadata": {"url": "https://docs.llamaindex.ai/en/stable/understanding/storing/storing/"}, "hash": "7af9c6741f3d00c7337ce16673e0be3159a5ed310ae8c2396d2fa11464049930", "class_name": "RelatedNodeInfo"}}, "text": "Once you have data loaded and indexed , you will probably want to store it to avoid the time and cost of re-indexing it. By default, your indexed data is stored only in memory. The simplest way to store your indexed data is to use the built-in .persist() method of every Index, which writes all the data to disk at the location specified. This works for any type of index. Here is an example of a Composable Graph: You can then avoid re-loading and re-indexing your data by loading the persisted index like this: Tip Important: if you had initialized your index with a custom transformations , embed_model , etc., you will need to pass in the same options during load_index_from_storage , or have it set as the global settings . As discussed in indexing , one of the most common types of Index is the VectorStoreIndex. The API calls to create the {ref} embeddings <what-is-an-embedding> in a VectorStoreIndex can be expensive in terms of time and money, so you will want to store them to avoid having to constantly re-index things. LlamaIndex supports a huge number of vector stores which vary in architecture, complexity and cost. In this example we'll be using Chroma, an open-source vector store. First you will need to install chroma: To use Chroma to store the embeddings from a VectorStoreIndex, you need to: Here's what that looks like, with a sneak peek at actually querying the data: If you've already created and stored your embeddings, you'll want to load them directly without loading your documents or creating a new VectorStoreIndex: Tip We have a more thorough example of using Chroma if you want to go deeper on this store. Now you have loaded data, indexed it, and stored that index, you're ready to query your data . If you've already created an index, you can add new documents to your index using the insert method. See the document management how-to for more details on managing documents and an example notebook.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf31bb82-68c1-468d-b326-a6126e701978": {"__data__": {"id_": "bf31bb82-68c1-468d-b326-a6126e701978", "embedding": null, "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}, "excluded_embed_metadata_keys": ["url"], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d16939f6-fac2-4f60-a672-8019936f7179", "node_type": "4", "metadata": {"url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}, "hash": "f3c6432df7e7c7e7b4af320b0d95d172b1099ab104063a55d3620323dd50f6c7", "class_name": "RelatedNodeInfo"}}, "text": "Alongside the rise of Large Language Models (LLM), has risen a swath of programming frameworks that aim to make it easier to build applications on top of them, such as LangChain and LlamaIndex . LlamaIndex is a programming framework that aims to simplify the development of LLM-enabled applications that leverage Retrieval Augmented Generation (RAG). A core conceptual component to understanding the mechanisms behind LlamaIndex is that of query Response Modes . LlamaIndex has 5 built-in Response Modes: compact, refine, tree_summarize, accumulation, and simple_summarize. In this article, I will: To evaluate each mode, I put them to the test in helping answer a question that has plagued the world since that fateful day in November 1963: Who shot President John F. Kennedy? I will demonstrate how each Response Mode works by asking LlamaIndex to summarize the conclusions of the \u2018 Warren Commission Report \u2019, the 250,000-word Congressional report that investigated the assassination of President Kennedy. This document far exceeds the token limit for GPT-4 and most other mainstream LLMs and requires the use of RAG techniques for an LLM to answer questions on it using only passed in contextual data (not training data). The concept of Retrieval Augmented Generation (RAG) describes an approach to allow an LLM to answer questions based on data that it wasn\u2019t originally trained on. In order to do this, an LLM must be fed this data as part of the prompt, which is generally referred to as \u2018context\u2019. However, LLMs have limited input size restrictions (also called token input size), which make it impossible and impractical to pass large datasets, such as the Warren Report, in their entirety via the prompt. Instead, with the RAG-approach, a query to an LLM is broken into two parts: a \u2018retrieval step\u2019 and then a \u2018generation step\u2019. The \u2018retrieval step\u2019 attempts to identify the portions of the original dataset that are most relevant to a user supplied query and to only pass this subset of data to an LLM, alongside the original query, as part of the \u2018generative step\u2019. Essentially, RAG is a mechanism to work within the input size restrictions of LLMs by only including in the prompt the most relevant parts of the dataset needed to answer a query. Usually, the \u2018retrieval\u2019 portion of RAG utilizes tried-and-true semantic search algorithms such as Cosine Similarity , alongside a Vector Databases to perform this step. For the purposes of this article and the testing of the Response Modes, the question I am seeking to get an answer to is: \u201cWhat are the conclusions of the Warren Report?\u201d For the purposes of this article, in the retrieval step I set my code to return the top 5 most relevant \u2018chunks\u2019 of data that relate to my original query from the Warren Report as part of the Cosine similarity algorithm. These 5 chunks of data are then passed forward to the LLM, which is where LlamaIndex Response Modes come into play. When building an LLM enabled application that utilizes RAG techniques, it is still likely that the subset of data returned as part of the retrieval step, which are normally referred to as \u2018chunks\u2019, will still be too large to fit within the input token limit for a single LLM call. Most likely, multiple calls will need to be made to the LLM in order to derive a single answer that utilizes all of the retrieved chunks. LlamaIndex Response Modes govern the various approaches that can be used to break down, sequence and combine the results of multiple LLM calls with these chunks to return a single answer to the original query. As of writing, there are 5 basic Response Modes that can be used when querying with LlamaIndex. In evaluating the 5 different Response Modes, I utilize the following frameworks and tools: For each test, I pose the same question: \u201cWhat are the main conclusions of the Warren Report regarding the assassination of President Kennedy?\u201d The following is the Python code I used to evaluate each Response Mode, this one configured to use the compact Response Mode: Note also that in my call to initialize the query_engine in LlamaIndex, I set the similarity_top_k parameter to 5, which tells LlamaIndex to return the top 5 chunks of data that are semantically similar to the query as part of the retrieval step. For all Response Modes, the same 5 chunks of text are returned from the Warren Commission Report, which are available for you to view in the table below: The Response Mode called compact is the default mode used by LlamaIndex if none is specified. The way the compact mode works is that for each chunk that is returned from the retrieval step, LlamaIndex concatenates as many of those chunks together into the largest possible string that fits into a single prompt to the GPT-4 LLM. In our example, the first 4 chunks of matched text fit into the context window for a single GPT-4 call, which means that it requires 2 LLM calls to answer our query on the conclusion of the Warren Report. The 1st call made using the compact Response Mode always uses the text_qa_template prompt: LlamaIndex then takes the answer returned to this prompt, the next concatenated set of chunks (in our case simply Chunk 5) and passes it to the LLM along with the last few sentences of Chunk 4 using the refine_template prompt: As you can see, the compact Response Mode doesn\u2019t answer the question at anything resembling a coherent answer. In fact, the LLM ends up throwing up its hands and doing a spot on rendition of a high school student fumbling their way trying to answer a question they simply have no clue about. The likely reason why the compact Response Mode was unable to answer the question is that the first 4 chunks of text from the Warren Report don\u2019t actually contain the conclusions of the report, which only appears in Chunk 5 (which oddly has a lower Cosine similarity score than the proceeding chunks). Thus, the structure of the refine_template is such that if the first set of calls has gone down the wrong path, it\u2019s difficult for the final prompt to steer the LLM back onto the right track. You can see the full-text log for both of the LLM calls made with the compact Response Mode below: The second Response Mode that LlamaIndex provides is refine . The refine mode is very similar to the compact Response Mode, except that instead of attempting to concatenate as many chunks as it can to maximize the use of the LLM token limit, LlamaIndex only include 1 chunk of retrieved data for each LLM call. Starting with the text_qa_template , LlamaIndex passes in Chunk 1 to the LLM. After that, LlamaIndex then progresses sequentially through each of the remaining chunks one at a time; with each subsequent LLM call using the refine_template to build upon the answer returned from the answer returned from the previous chunk. While the compact Response Mode tried to obfuscate it\u2019s inability to answer the question behind a wall of hand-waving text, the refine Response Mode yields a much more succinct, yet equally useless answer. Much like the compact mode, the refine is much more likely to not be able to properly answer the question when the initial chunks passed to it are less relevant. In our case, the first chunk of data from the Warren Report is the introduction and forward parts of the report which do not contain any conclusions whatsoever, which is likely why it was never able to return a proper answer to our query. You can see the full-text log for all of the LLM calls made with the refine Response Mode below: The third, and by far most effective, Response Mode is tree_summarize. The astute reader might surmise from the use of the word \u2018tree\u2019 that there is a recursive property to this response mode, and they would be correct. The tree_summarize mode in its base case makes a series of LLM calls that concatenate chunks of retrieved data so that it maximizes the input token limit for the LLM. It then takes the outputs of each of these base case responses, and then passes them together to the LLM and instructs it to derive an answer using those initial answers as context. For anyone familiar working with LangChain, the tree_summarize Response Mode is essentially the same as LangChain\u2019s MapReduceDocumentChain . In our analysis of the Warren Report, the tree_summarize mode required 3 calls to the LLM, 2 for processing the 5 chunks of data, and then 1 for combining the answers from the first 2. The prompt template used by the tree_summarize Response Mode is quite simple and the same each time, regardless if we are working at the \u2018leaf\u2019 level or higher up in the processing tree: After making a similar call that includes only Chunk 5, tree_summarize then uses the following prompt template to combine the answers: The tree_summarize response mode nails the answer and delivers a thoughtful and complete summary of the findings of the Warren Report that correctly identify Lee Harvey Oswald as the assassin, while also in the same breath, disabusing any notion of a conspiracy or second shooter. Whereas the refine and compact modes were led astray by the irrelevance of the first set of chunks they analyzed, tree_summarize mode overcomes this as it is uses a map-reduce pattern to have the LLM independently analyze each concatenated chunk of data and then in a separate prompt combine the outputs of those into a single answer. You can see the full-text log for all of the LLM calls made using the tree_summarize Response Mode below: The accumulate Response Mode is quite simple, LlamaIndex makes 1 call per retrieved chunk and then returns every \u2018non-null\u2019 answer together as an array of answers. For each of the calls that LlamaIndex makes in accumulate mode, it uses a similar template to tree_summarize : The returned result from LlamaIndex is an array of strings of length 2, containing the responses returned from the LLM for chunks 4 and 5. The answers for chunks 1,2,3 are not included in this result, because for each of those calls, the LLM returned the logical equivalent of a \u2018null\u2019 response in the form of 'The context does not provide information on the main conclusions of the Warren Report regarding the assassination of President Kenne dy.\u2019 The accumulate mode doesn\u2019t so much answer the question, but instead returns n answers to the question with each of the answers being scoped simply to the context chunk passed to the LLM in that call. It is the responsibility then of the calling application to take the list of answers to produce an actual final answer to the query. \u2018Accumulate\u2019 doesn\u2019t work well for something like the Warren Report where each chunk doesn\u2019t necessarily contain the complete information to answer the question. You can see the full-text log for all of the LLM calls made using the accumulate Response Mode below: The final LlamaIndex Response Mode is simple_summarize . This perhaps the most basic and straightforward of the response modes. In this mode, LlamaIndex truncates all text chunks so that all chunks can be concatenated and passed into the LLM in a single call. No matter how many chunks are retrieved, there will only ever be a single call made to the LLM. In our example, the prompt template that is used in simple_summarize mode looks like: Surprisingly, the answer provided by simple_summarize is not bad and almost as complete as the one provided by tree_summarize However, I would attribute this more to luck then a structural advantage of the mechanism. With the simple_summarize mode, as the number of chunks returned from the retrieval step goes up, its likely that the ultimate answer returned will decrease in quality due to the knowledge being lost in the truncated segments of each chunk. You can view the full-text log of the prompt made in the simple_summarize test here . Looking at the results of my tests, the tree_summarize Response Mode returned the most comprehensive and complete answer to the question \u201cWhat are the conclusions of the Warren Report?\u201d. The recursive, map-reduce like algorithm it employs allows it to build up the correct answer looking at all matched chunks of text from the Warren Commission Report and the responses from GPT-4 to each of them. This is not to say that tree_summarize is the only Response Mode you need when building a RAG-style application, however for the purposes of summarizing content from across a large body of text, it clearly has structural advantages that lend itself more effective than the other modes. However, it\u2019s important to understand the different motivations behind each of the other 4 Response Modes and to know which circumstances each might be the best tool for the job. With the rapidly increasing sizes of input token limits for newer LLM models, it can be argued that the need for RAG will slowly diminish. However, even in a world where LLMs are able to accept million token inputs, it will always behoove a consumer of an LLM to maximize the efficiency of the each token used in the context passed in each LLM call. As such, the LlamaIndex framework provides a very neat and consistent programming model to build RAG-style applications, certainly more so than LangChain. For those of you interested in learning more about LlamaIndex and building with it, I encourage you to visit the LlamaIndex documentation , as the team has done an excellent job of building a set of , clear and easy to work through tutorials that outline the many capabilities of LlamaIndex. Bobby Gill is the co-founder and Chief Architect of BlueLabel, an award winning digital product agency headquartered in New York. With over two decades of experience in software development, he is a seasoned full-stack engineer, software architect, and AI practitioner. Bobby currently leads the BlueLabel AI/ML practice, where he is leading a team of engineers operationalizing the transformational capabilities of generative AI within BlueLabel and for a number of enterprise clients. A Digital Transformation Agency. 4.8/5 Overall Rating \u00a9 2024 BlueLabel | All Rights Reserved \u2013 Total Rating 4.8 out of 5 based on 40+ reviews", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"23529a5c-c600-4b4f-875d-1741a9dcddff": {"doc_hash": "cc5146a461c8f8ddc378b95136461ecce37fb226637a5f3e9219b1d107a5923c", "ref_doc_id": "531b7d2d-ce99-4996-85d5-a8ce66b75f3f"}, "bf31bb82-68c1-468d-b326-a6126e701978": {"doc_hash": "59931df934b914ba23da8a4b9be689ee88fb6f929e13d833884ed6bad7c66858", "ref_doc_id": "d16939f6-fac2-4f60-a672-8019936f7179"}}, "docstore/ref_doc_info": {"531b7d2d-ce99-4996-85d5-a8ce66b75f3f": {"node_ids": ["23529a5c-c600-4b4f-875d-1741a9dcddff"], "metadata": {"tag": "p", "url": "https://docs.llamaindex.ai/en/stable/understanding/storing/storing/"}}, "d16939f6-fac2-4f60-a672-8019936f7179": {"node_ids": ["bf31bb82-68c1-468d-b326-a6126e701978"], "metadata": {"tag": "p", "url": "https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/"}}}}